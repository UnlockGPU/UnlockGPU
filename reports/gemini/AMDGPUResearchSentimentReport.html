<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=DFQxm4rd7fRHgM9OTejWVT5Vho6BE7M80rHXEVKqXWegg2XYR88pwOsaJkfiF7cJu5e0vFtnyLdhsxviZUUN-U0KZVwUvSK-LyXz4qcE1hc);.lst-kix_list_c-0>li:before{content:"" counter(lst-ctn-kix_list_c-0,decimal) ". "}ol.lst-kix_list_7-0{list-style-type:none}ul.lst-kix_list_b-8{list-style-type:none}.lst-kix_list_c-1>li:before{content:"\0025cb   "}ol.lst-kix_list_9-0.start{counter-reset:lst-ctn-kix_list_9-0 0}ul.lst-kix_list_b-3{list-style-type:none}ul.lst-kix_list_b-2{list-style-type:none}.lst-kix_list_c-3>li:before{content:"\0025a0   "}.lst-kix_list_c-4>li:before{content:"\0025a0   "}ul.lst-kix_list_b-1{list-style-type:none}ul.lst-kix_list_b-0{list-style-type:none}ul.lst-kix_list_b-7{list-style-type:none}ul.lst-kix_list_b-6{list-style-type:none}.lst-kix_list_c-2>li:before{content:"\0025a0   "}ul.lst-kix_list_b-5{list-style-type:none}ul.lst-kix_list_b-4{list-style-type:none}ul.lst-kix_list_9-3{list-style-type:none}ul.lst-kix_list_9-4{list-style-type:none}ul.lst-kix_list_9-1{list-style-type:none}ul.lst-kix_list_9-2{list-style-type:none}ul.lst-kix_list_9-7{list-style-type:none}ul.lst-kix_list_9-8{list-style-type:none}ul.lst-kix_list_9-5{list-style-type:none}.lst-kix_list_5-0>li{counter-increment:lst-ctn-kix_list_5-0}.lst-kix_list_7-0>li{counter-increment:lst-ctn-kix_list_7-0}.lst-kix_list_9-0>li{counter-increment:lst-ctn-kix_list_9-0}ul.lst-kix_list_9-6{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}.lst-kix_list_5-0>li:before{content:"" counter(lst-ctn-kix_list_5-0,decimal) ". "}ol.lst-kix_list_6-0{list-style-type:none}.lst-kix_list_5-3>li:before{content:"\0025a0   "}ul.lst-kix_list_a-4{list-style-type:none}ul.lst-kix_list_a-3{list-style-type:none}.lst-kix_list_5-2>li:before{content:"\0025a0   "}ul.lst-kix_list_a-2{list-style-type:none}ul.lst-kix_list_a-1{list-style-type:none}.lst-kix_list_5-1>li:before{content:"\0025cb   "}ul.lst-kix_list_a-8{list-style-type:none}ul.lst-kix_list_a-7{list-style-type:none}ul.lst-kix_list_a-6{list-style-type:none}ul.lst-kix_list_a-5{list-style-type:none}.lst-kix_list_5-7>li:before{content:"\0025a0   "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}.lst-kix_list_5-6>li:before{content:"\0025a0   "}.lst-kix_list_5-8>li:before{content:"\0025a0   "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_5-4>li:before{content:"\0025a0   "}.lst-kix_list_5-5>li:before{content:"\0025a0   "}ul.lst-kix_list_8-0{list-style-type:none}ul.lst-kix_list_8-1{list-style-type:none}.lst-kix_list_c-0>li{counter-increment:lst-ctn-kix_list_c-0}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_6-1>li:before{content:"\0025cb   "}.lst-kix_list_6-3>li:before{content:"\0025a0   "}.lst-kix_list_6-0>li:before{content:"" counter(lst-ctn-kix_list_6-0,decimal) ". "}.lst-kix_list_6-4>li:before{content:"\0025a0   "}.lst-kix_list_3-0>li{counter-increment:lst-ctn-kix_list_3-0}ol.lst-kix_list_4-0.start{counter-reset:lst-ctn-kix_list_4-0 0}.lst-kix_list_6-2>li:before{content:"\0025a0   "}.lst-kix_list_c-7>li:before{content:"\0025a0   "}.lst-kix_list_c-5>li:before{content:"\0025a0   "}.lst-kix_list_c-8>li:before{content:"\0025a0   "}.lst-kix_list_6-8>li:before{content:"\0025a0   "}.lst-kix_list_6-5>li:before{content:"\0025a0   "}.lst-kix_list_6-7>li:before{content:"\0025a0   "}.lst-kix_list_c-6>li:before{content:"\0025a0   "}.lst-kix_list_6-6>li:before{content:"\0025a0   "}.lst-kix_list_2-7>li:before{content:"\0025a0   "}.lst-kix_list_7-4>li:before{content:"\0025a0   "}.lst-kix_list_7-6>li:before{content:"\0025a0   "}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-5>li:before{content:"\0025a0   "}.lst-kix_list_b-8>li:before{content:"\0025a0   "}.lst-kix_list_7-2>li:before{content:"\0025a0   "}.lst-kix_list_b-4>li:before{content:"\0025a0   "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ol.lst-kix_list_3-0.start{counter-reset:lst-ctn-kix_list_3-0 0}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_7-8>li:before{content:"\0025a0   "}ul.lst-kix_list_3-5{list-style-type:none}ol.lst-kix_list_9-0{list-style-type:none}.lst-kix_list_b-6>li:before{content:"\0025a0   "}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_b-0>li:before{content:"\0025cf   "}.lst-kix_list_4-1>li:before{content:"\0025cb   "}.lst-kix_list_b-2>li:before{content:"\0025a0   "}.lst-kix_list_9-2>li:before{content:"\0025a0   "}.lst-kix_list_4-3>li:before{content:"\0025a0   "}.lst-kix_list_4-5>li:before{content:"\0025a0   "}.lst-kix_list_9-0>li:before{content:"" counter(lst-ctn-kix_list_9-0,decimal) ". "}ul.lst-kix_list_c-8{list-style-type:none}ul.lst-kix_list_c-7{list-style-type:none}.lst-kix_list_9-6>li:before{content:"\0025a0   "}ul.lst-kix_list_c-2{list-style-type:none}ul.lst-kix_list_c-1{list-style-type:none}.lst-kix_list_9-4>li:before{content:"\0025a0   "}ul.lst-kix_list_c-6{list-style-type:none}.lst-kix_list_a-0>li:before{content:"" counter(lst-ctn-kix_list_a-0,decimal) ". "}ul.lst-kix_list_c-5{list-style-type:none}ul.lst-kix_list_c-4{list-style-type:none}ul.lst-kix_list_c-3{list-style-type:none}ul.lst-kix_list_2-8{list-style-type:none}ul.lst-kix_list_2-2{list-style-type:none}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_9-8>li:before{content:"\0025a0   "}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb   "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025a0   "}.lst-kix_list_1-7>li:before{content:"\0025a0   "}.lst-kix_list_1-5>li:before{content:"\0025a0   "}ol.lst-kix_list_a-0{list-style-type:none}.lst-kix_list_2-1>li:before{content:"\0025cb   "}ol.lst-kix_list_6-0.start{counter-reset:lst-ctn-kix_list_6-0 0}.lst-kix_list_2-3>li:before{content:"\0025a0   "}ol.lst-kix_list_3-0{list-style-type:none}.lst-kix_list_3-0>li:before{content:"" counter(lst-ctn-kix_list_3-0,decimal) ". "}ul.lst-kix_list_5-7{list-style-type:none}ul.lst-kix_list_5-8{list-style-type:none}.lst-kix_list_3-1>li:before{content:"\0025cb   "}.lst-kix_list_3-2>li:before{content:"\0025a0   "}ul.lst-kix_list_5-5{list-style-type:none}ol.lst-kix_list_c-0.start{counter-reset:lst-ctn-kix_list_c-0 0}ul.lst-kix_list_5-6{list-style-type:none}.lst-kix_list_8-1>li:before{content:"\0025cb   "}.lst-kix_list_4-0>li{counter-increment:lst-ctn-kix_list_4-0}.lst-kix_list_8-2>li:before{content:"\0025a0   "}.lst-kix_list_6-0>li{counter-increment:lst-ctn-kix_list_6-0}.lst-kix_list_3-5>li:before{content:"\0025a0   "}.lst-kix_list_3-4>li:before{content:"\0025a0   "}ul.lst-kix_list_5-3{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025a0   "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}.lst-kix_list_8-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-2{list-style-type:none}.lst-kix_list_8-7>li:before{content:"\0025a0   "}.lst-kix_list_3-8>li:before{content:"\0025a0   "}.lst-kix_list_8-5>li:before{content:"\0025a0   "}.lst-kix_list_a-7>li:before{content:"\0025a0   "}.lst-kix_list_8-6>li:before{content:"\0025a0   "}.lst-kix_list_2-0>li{counter-increment:lst-ctn-kix_list_2-0}.lst-kix_list_8-3>li:before{content:"\0025a0   "}.lst-kix_list_3-6>li:before{content:"\0025a0   "}.lst-kix_list_3-7>li:before{content:"\0025a0   "}.lst-kix_list_a-8>li:before{content:"\0025a0   "}.lst-kix_list_8-4>li:before{content:"\0025a0   "}ol.lst-kix_list_5-0.start{counter-reset:lst-ctn-kix_list_5-0 0}.lst-kix_list_a-1>li:before{content:"\0025cb   "}.lst-kix_list_a-2>li:before{content:"\0025a0   "}.lst-kix_list_a-3>li:before{content:"\0025a0   "}.lst-kix_list_a-5>li:before{content:"\0025a0   "}.lst-kix_list_a-6>li:before{content:"\0025a0   "}.lst-kix_list_a-4>li:before{content:"\0025a0   "}.lst-kix_list_8-8>li:before{content:"\0025a0   "}ol.lst-kix_list_2-0{list-style-type:none}.lst-kix_list_4-8>li:before{content:"\0025a0   "}.lst-kix_list_4-7>li:before{content:"\0025a0   "}ul.lst-kix_list_4-8{list-style-type:none}ul.lst-kix_list_4-6{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ul.lst-kix_list_4-1{list-style-type:none}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}ol.lst-kix_list_c-0{list-style-type:none}.lst-kix_list_7-0>li:before{content:"" counter(lst-ctn-kix_list_7-0,decimal) ". "}ol.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_2-6>li:before{content:"\0025a0   "}.lst-kix_list_2-4>li:before{content:"\0025a0   "}.lst-kix_list_2-8>li:before{content:"\0025a0   "}.lst-kix_list_7-1>li:before{content:"\0025cb   "}.lst-kix_list_7-5>li:before{content:"\0025a0   "}.lst-kix_list_7-3>li:before{content:"\0025a0   "}ul.lst-kix_list_7-5{list-style-type:none}ul.lst-kix_list_7-6{list-style-type:none}.lst-kix_list_b-5>li:before{content:"\0025a0   "}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_b-3>li:before{content:"\0025a0   "}.lst-kix_list_b-7>li:before{content:"\0025a0   "}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ol.lst-kix_list_a-0.start{counter-reset:lst-ctn-kix_list_a-0 0}ul.lst-kix_list_7-1{list-style-type:none}ul.lst-kix_list_7-2{list-style-type:none}.lst-kix_list_7-7>li:before{content:"\0025a0   "}.lst-kix_list_4-0>li:before{content:"" counter(lst-ctn-kix_list_4-0,decimal) ". "}.lst-kix_list_b-1>li:before{content:"\0025cb   "}.lst-kix_list_4-4>li:before{content:"\0025a0   "}.lst-kix_list_4-2>li:before{content:"\0025a0   "}.lst-kix_list_4-6>li:before{content:"\0025a0   "}.lst-kix_list_9-3>li:before{content:"\0025a0   "}ol.lst-kix_list_7-0.start{counter-reset:lst-ctn-kix_list_7-0 0}.lst-kix_list_9-1>li:before{content:"\0025cb   "}ol.lst-kix_list_4-0{list-style-type:none}.lst-kix_list_9-7>li:before{content:"\0025a0   "}.lst-kix_list_9-5>li:before{content:"\0025a0   "}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ". "}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_a-0>li{counter-increment:lst-ctn-kix_list_a-0}ul.lst-kix_list_6-3{list-style-type:none}.lst-kix_list_1-2>li:before{content:"\0025a0   "}ol.lst-kix_list_2-0.start{counter-reset:lst-ctn-kix_list_2-0 0}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"\0025a0   "}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li:before{content:"\0025a0   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_2-0>li:before{content:"" counter(lst-ctn-kix_list_2-0,decimal) ". "}.lst-kix_list_1-8>li:before{content:"\0025a0   "}.lst-kix_list_2-2>li:before{content:"\0025a0   "}ol{margin:0;padding:0}table td,table th{padding:0}.c16{border-right-style:solid;padding:6pt 9pt 6pt 9pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#f8fafd;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:6pt 9pt 6pt 9pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#f8fafd;border-left-style:solid;border-bottom-width:1pt;width:156pt;border-top-color:#000000;border-bottom-style:solid}.c2{-webkit-text-decoration-skip:none;color:#0000ee;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Google Sans"}.c0{color:#1b1c1d;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Google Sans Text";font-style:normal}.c12{color:#575b5f;font-weight:400;text-decoration:none;vertical-align:super;font-size:12pt;font-family:"Google Sans Text";font-style:normal}.c6{color:#1b1c1d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Google Sans Text";font-style:normal}.c13{color:#1b1c1d;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Google Sans";font-style:normal}.c24{margin-left:24pt;padding-top:0pt;padding-bottom:6pt;line-height:1.149999976158142;padding-left:0pt;text-align:left}.c18{margin-left:44.2pt;padding-top:0pt;padding-bottom:6pt;line-height:1.149999976158142;padding-left:0pt;text-align:left}.c15{vertical-align:super;font-size:12pt;font-family:"Google Sans Text";font-style:normal;color:#575b5f;font-weight:400}.c1{margin-left:30pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;padding-left:0pt;text-align:left}.c23{margin-left:44.2pt;padding-top:6pt;padding-bottom:6pt;line-height:1.149999976158142;padding-left:0pt;text-align:left}.c29{margin-left:24pt;padding-top:6pt;padding-bottom:6pt;line-height:1.149999976158142;padding-left:0pt;text-align:left}.c27{margin-left:23.2pt;padding-top:6pt;padding-bottom:6pt;line-height:1.149999976158142;padding-left:0pt;text-align:left}.c34{color:#000000;font-weight:700;text-decoration:none;font-size:12pt;font-family:"Google Sans"}.c19{color:#1b1c1d;font-weight:700;font-size:12pt;font-family:"Google Sans";font-style:normal}.c9{font-size:12pt;font-family:"Google Sans Text";font-style:normal;color:#1b1c1d;font-weight:700}.c33{font-size:12pt;font-family:"Google Sans Text";font-style:italic;color:#1b1c1d;font-weight:400}.c35{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial"}.c8{font-size:12pt;font-family:"Google Sans Text";font-style:normal;color:#1b1c1d;font-weight:400}.c10{font-size:10pt;font-family:"Google Sans Text";font-style:normal;color:#1b1c1d;font-weight:400}.c32{color:#1b1c1d;font-weight:700;text-decoration:none;font-size:16pt;font-family:"Google Sans"}.c31{padding-top:0pt;padding-bottom:12.8pt;line-height:1.0;text-align:left}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.149999976158142;text-align:left}.c36{padding-top:12pt;padding-bottom:6pt;line-height:1.149999976158142;text-align:left}.c26{padding-top:24pt;padding-bottom:12pt;line-height:1.149999976158142;text-align:left}.c28{padding-top:12pt;padding-bottom:12pt;line-height:1.149999976158142;text-align:left}.c3{padding-top:0pt;padding-bottom:12pt;line-height:1.149999976158142;text-align:left}.c30{border-spacing:0;border-collapse:collapse;margin-right:auto}.c38{padding-top:12pt;padding-bottom:12.8pt;line-height:1.149999976158142;text-align:left}.c25{padding-top:0pt;padding-bottom:6pt;line-height:1.149999976158142;text-align:left}.c37{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{font-size:12pt;font-weight:400;font-family:"Google Sans"}.c4{color:inherit;text-decoration:inherit}.c22{vertical-align:baseline;font-style:normal}.c17{text-decoration:none;vertical-align:baseline}.c21{padding:0;margin:0}.c39{margin-left:23.2pt;padding-left:0pt}.c7{height:0pt}.c14{height:11pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:12pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:12pt;font-family:"Arial";line-height:1.0;text-align:left}h2{padding-top:11.2pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:11.2pt;font-family:"Arial";line-height:1.0;text-align:left}h3{padding-top:12pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:12pt;font-family:"Arial";line-height:1.0;text-align:left}h4{padding-top:12.8pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.0;text-align:left}h5{padding-top:12.8pt;color:#000000;font-weight:700;font-size:9pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.0;text-align:left}h6{padding-top:18pt;color:#000000;font-weight:700;font-size:8pt;padding-bottom:18pt;font-family:"Arial";line-height:1.0;text-align:left}</style></head><body class="c37 doc-content"><p class="c3 c14"><span class="c22 c35"></span></p><h1 class="c25"><span class="c32 c22">A Comprehensive Background Report on Researcher Experiences and Sentiment Toward AMD GPU Software and Hardware in High-Performance Computing Environments</span></h1><p class="c3 c14"><span class="c32 c22"></span></p><p class="c3 c14"><span class="c22 c32"></span></p><h2 class="c25"><span class="c13">Executive Summary</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3"><span class="c6">This report provides a comprehensive analysis of researcher experiences and sentiment regarding AMD GPU software and hardware, particularly focusing on their adoption within academic, scientific, and high-performance computing (HPC) environments from 2022 to 2025, with historical context from 2018 to 2021.</span></p><p class="c3"><span class="c8">AMD has achieved significant strategic successes in top-tier HPC, powering exascale systems like Frontier and El Capitan, which consistently rank as the world&#39;s most powerful supercomputers.</span><span class="c15">1</span><span class="c8">&nbsp;This demonstrates robust hardware capabilities and a foundational ROCm software stack that performs effectively in highly controlled, large-scale environments. The ROCm software platform is also rapidly maturing, with recent versions (6.4, 7.0) delivering substantial performance improvements&mdash;up to 3.5x for inference and 3x for training&mdash;alongside expanded compatibility with major frameworks such as PyTorch, TensorFlow, and Hugging Face.</span><span class="c15">6</span><span class="c8">&nbsp;The anticipated availability of ROCm on Windows in the second half of 2025 is a crucial development aimed at broadening accessibility.</span><span class="c12">6</span></p><p class="c3"><span class="c8">Despite these high-profile achievements and continuous software advancements, a prevailing negative sentiment persists among individual researchers and smaller institutions. This is largely attributed to ongoing challenges related to complex and unstable installation processes, driver issues, and limited official support for consumer-grade GPUs.</span><span class="c15">15</span><span class="c8">&nbsp;The entrenched &quot;CUDA moat&quot; remains a significant barrier; while AMD&#39;s Heterogeneous-Compute Interface for Portability (HIP) aims to facilitate code porting, direct equivalents for all CUDA features are often absent, necessitating manual intervention and contributing to a steep learning curve and developer reluctance.</span><span class="c15">15</span><span class="c8">&nbsp;NVIDIA&#39;s established ecosystem and widespread availability in cloud rental markets further solidify its dominant position.</span><span class="c12">28</span></p><p class="c3"><span class="c8">Institutional IT departments and procurement offices also encounter difficulties when integrating AMD GPUs, stemming from legacy infrastructure, stringent power and cooling requirements, and the inherent complexities of heterogeneous systems.</span><span class="c15">32</span><span class="c8">&nbsp;Nevertheless, AMD&#39;s academic programs and direct engineering support have facilitated successful deployments in certain large supercomputing centers.</span><span class="c12">34</span></p><p class="c3"><span class="c8">Strategically, AMD&#39;s open-source approach to ROCm represents a key differentiator, appealing to researchers who prioritize flexibility and wish to avoid vendor lock-in.</span><span class="c15">3</span><span class="c6">&nbsp;However, the practical usability issues faced by a broader user base undermine this advantage. To genuinely challenge NVIDIA&#39;s ecosystem dominance, AMD must prioritize enhancing the out-of-the-box experience, expanding official support for a wider array of GPUs (especially consumer cards vital for academic exploration), and improving documentation alongside transparent continuous integration/continuous delivery (CI/CD) processes. Addressing these pain points is paramount for cultivating a loyal developer community, particularly among early-career researchers and smaller institutions, which are crucial for long-term adoption and ecosystem expansion.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Introduction</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3"><span class="c6">This report aims to provide a comprehensive background on researcher experiences and sentiment concerning AMD GPU software and hardware. The analysis specifically targets adoption trends, use cases, persistent challenges, and notable successes within academic, scientific, and high-performance computing (HPC) environments. The primary temporal focus spans from 2022 to 2025, capturing recent developments and user feedback, while historical context from 2018 to 2021 is incorporated to discern long-term trends and evolutionary trajectories.</span></p><p class="c3"><span class="c6">The investigation delves into several critical areas: the adoption rates of AMD&#39;s ROCm and OpenCL platforms; their compatibility with prevalent scientific computing frameworks such as TensorFlow, PyTorch, GROMACS, LAMMPS, and OpenMM; the performance and scalability demonstrated in cluster and supercomputer environments; the stability of drivers and the overall software stack for demanding research workloads; the quality and accessibility of documentation and support for advanced use cases; the practical experiences associated with grant-funded hardware acquisitions and institutional IT support; and the inherent barriers encountered when attempting to port existing CUDA/NVIDIA-based research code to AMD platforms.</span></p><p class="c3"><span class="c6">The methodology employed for this report involves a rigorous synthesis of information derived from a diverse array of sources. These include academic forums and mailing lists, such as Reddit&#39;s r/ROCm community and the JuliaLang Discourse; GitHub issues and discussions pertinent to major scientific software projects like GROMACS, OpenMM, and components of the ROCm ecosystem (e.g., rocprofiler-compute); user documentation and proceedings from supercomputing centers; academic conference proceedings (e.g., SC, ISC); scholarly articles and preprints available on platforms such as ResearchGate and arXiv; and official AMD corporate blogs and technical documentation.</span></p><p class="c3"><span class="c6">The analytical framework guiding this report encompasses several dimensions: sentiment analysis, categorizing opinions as positive, neutral, or negative with supporting rationale; frequency analysis of reported pain points to identify the most pervasive issues; collection of researcher-proposed solutions and improvements; documentation of success stories and instances where AMD GPUs have demonstrated efficacy; trend analysis to compare recent and historical sentiment shifts; and, where data permits, the extraction of demographic insights related to research fields, geographical regions, and institutional types. A core tenet of this analysis is to prioritize recent, technically detailed feedback over generic complaints, ensuring a balanced perspective that incorporates both positive and negative experiences, and focusing on actionable, specific issues. Claims are verified against technical documentation or published results where feasible, and a clear distinction is maintained between hardware-related and software-related challenges.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">ROCm and OpenCL Adoption in Research and HPC Environments</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Current Adoption Landscape (2022-2025): Growth and Key Deployments</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD has significantly expanded its presence within the global supercomputing landscape in recent years. As of June 2025, AMD-powered systems constitute 34% of the latest Top500 list, totaling 172 systems. This represents a substantial increase in market penetration compared to previous periods.</span><span class="c15">1</span><span class="c8">&nbsp;Notably, AMD accelerators power the world&#39;s two most powerful supercomputers, El Capitan and Frontier, which consistently hold the No. 1 and No. 2 positions, respectively. El Capitan is built on AMD Instinct MI300A accelerated processing units (APUs), while Frontier leverages AMD Instinct MI250X accelerators.</span><span class="c15">1</span><span class="c6">&nbsp;These flagship deployments underscore AMD&#39;s capability in delivering top-tier performance for the most demanding HPC and AI workloads.</span></p><p class="c3"><span class="c8">Beyond these premier systems, AMD Instinct accelerators are increasingly adopted in various HPC and AI clusters worldwide. Examples include LUMI in Finland, recognized as Europe&#39;s fastest supercomputer; HPC6 in Italy; and Setonix in Australia, the country&#39;s most powerful and energy-efficient supercomputer, all of which operate within the ROCm ecosystem.</span><span class="c15">3</span><span class="c6">&nbsp;This widespread adoption in national and regional supercomputing centers highlights the growing confidence in AMD&#39;s hardware and software stack for large-scale scientific discovery.</span></p><p class="c3"><span class="c8">Academic institutions are also actively engaging with AMD&#39;s technology. The University of Oregon, in collaboration with MIT and Stanford, hosted a hackathon providing early access to MI300A APUs. This event facilitated successful initial code porting and research projects spanning diverse fields such as computational fluid dynamics, climate science, and artificial intelligence.</span><span class="c15">41</span><span class="c8">&nbsp;Similarly, Goethe University in Europe is utilizing one of the continent&#39;s largest AMD GPU clusters to accelerate AI-driven physics and digital twin models, further demonstrating the increasing integration of AMD solutions into academic research infrastructure.</span><span class="c12">42</span></p><p class="c3"><span class="c8">To democratize access to its high-performance computing resources, AMD has launched the AMD Developer Cloud. This platform offers instant, hardware-free access to AMD Instinct MI300X GPUs, complete with pre-configured ROCm environments and complimentary credits. This initiative is designed to lower the barrier to entry for individual developers and open-source contributors, enabling broader experimentation and development on AMD hardware.</span><span class="c12">6</span></p><p class="c3 c14"><span class="c12"></span></p><h3 class="c25"><span class="c19 c17">Historical Trajectory and Ecosystem Evolution (2018-2021)</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">The trajectory of AMD&#39;s HPC adoption reveals a strategic shift in focus. Historically, AMD&#39;s presence in the Top500 list was more modest. For instance, in November 2020, AMD powered 21 supercomputers, a number that grew to 73 by November 2021.</span><span class="c15">44</span><span class="c8">&nbsp;During this earlier period, NVIDIA GPUs were more prevalent in top-tier systems, as exemplified by the Summit supercomputer at Oak Ridge National Laboratory, which launched in 2018 and was recognized as the world&#39;s leading AI for science supercomputer for several years.</span><span class="c12">45</span></p><p class="c3"><span class="c8">The evolution of the ROCm software stack during 2018-2021 was foundational to AMD&#39;s current successes. ROCm 1.0, released around 2018, showcased initial capabilities for porting CUDA code to HIP.</span><span class="c15">11</span><span class="c8">&nbsp;Subsequent versions introduced critical features: ROCm 2.0 (2019) integrated Linux Kernel upstream support and the MIOpen deep learning libraries; ROCm 3.0 (2020) added support for AMD Infinity Fabric technology and released RCCL communication libraries, while also expanding the ecosystem to include cluster management and deployment tools like Docker, Kubernetes, and SLURM.</span><span class="c15">11</span><span class="c8">&nbsp;ROCm 4.0 (2021) brought support for AMD&#39;s CDNA architecture, and PyTorch official package support became available with ROCm 5.0.</span><span class="c12">11</span></p><p class="c3"><span class="c8">Parallel to software development, efforts were made to enhance foundational libraries for large-scale HPC. The MVAPICH2-GDR library, for example, added support for AMD GPUs starting in 2020, demonstrating a commitment to building a robust software infrastructure for distributed computing.</span><span class="c12">47</span></p><p class="c3"><span class="c8">The observed increase in AMD&#39;s Top500 presence, particularly the capture of the top two exascale systems (Frontier and El Capitan), represents a deliberate and successful strategic pivot. This is not merely an incremental expansion but a significant repositioning of AMD as a primary provider for national-scale, flagship HPC systems. These highly visible, large-scale contracts serve as crucial validation points and reference architectures, demonstrating AMD&#39;s capabilities to the broader HPC community. The open-source nature of ROCm likely contributes to its appeal among large, publicly funded institutions that value transparency, flexibility, and control over their software stacks.</span><span class="c15">3</span><span class="c6">&nbsp;This strategic focus on securing top-tier HPC deployments appears to be a cornerstone of AMD&#39;s market penetration.</span></p><p class="c3"><span class="c9 c17">Table: Historical AMD GPU Adoption in TOP500 (2018-2025)</span></p><table class="c30"><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Year (TOP500 List Date)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Total AMD Systems</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Highest AMD Rank</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Key AMD-Powered Systems</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">June 2018</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not in top 2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">Summit (NVIDIA) was #1 </span><span class="c12">45</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Dec 2018</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not in top 2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">(ROCm 1.9.1 released) </span><span class="c12">50</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Jan 2019</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not in top 2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">(ROCm 2.0 released) </span><span class="c12">50</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Jun 2019</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not in top 2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">(ROCm 2.0 performance noted) </span><span class="c12">50</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Nov 2020</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">21</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">(ROCm 4.0 released) </span><span class="c12">44</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Nov 2021</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">73</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Not specified</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">Frontier (deployment ongoing) </span><span class="c12">44</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">June 2022</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">101 (AMD64-based)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">#2 (Frontier)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">Frontier </span><span class="c12">4</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Nov 2024</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">172</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">#1 (El Capitan)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">El Capitan, Frontier, LUMI, HPC6 </span><span class="c12">3</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">June 2025</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">172 (34%)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">#1 (El Capitan)</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">El Capitan, Frontier </span><span class="c12">1</span></p></td></tr></table><p class="c26"><span class="c33 c17">Note: Data for &quot;Total AMD Systems&quot; prior to June 2022 often refers to CPU count rather than GPU count on the TOP500 list, but the trend of increasing AMD presence is consistent. The &quot;Highest AMD Rank&quot; specifically refers to GPU-accelerated systems where AMD GPUs are the primary accelerators.</span></p><p class="c3 c14"><span class="c33 c17"></span></p><h2 class="c25"><span class="c13">Technical Performance and Scalability Analysis</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Compatibility with Scientific Computing Frameworks</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">ROCm provides official support for prominent AI/machine learning frameworks, including TensorFlow and PyTorch. AMD actively maintains dedicated ROCm repositories for these frameworks to ensure timely bug fixes, updates, and compatibility with the latest ROCm versions.</span><span class="c15">11</span><span class="c8">&nbsp;Furthermore, models from Hugging Face are also supported within the ROCm ecosystem.</span><span class="c12">11</span></p><p class="c3"><span class="c8">In the domain of molecular dynamics, LAMMPS is optimized for ROCm, leveraging the Kokkos programming model and HIP FFT for accelerated computations.</span><span class="c15">13</span><span class="c8">&nbsp;GROMACS has also seen significant development in its SYCL backend to target AMD GPUs, with experimental HIP support introduced in GROMACS 2025. This allows GROMACS to utilize AMD hardware for various simulations.</span><span class="c15">58</span><span class="c8">&nbsp;OpenMM, another molecular simulation toolkit, is available via ROCm containers and includes HIP platform support.</span><span class="c15">21</span><span class="c8">&nbsp;However, some compatibility issues have been observed with specific ROCm versions and RDNA2 cards in OpenMM, indicating that while general support exists, granular compatibility can be a challenge.</span><span class="c12">69</span></p><p class="c3"><span class="c8">The hipify tool is designed to streamline the conversion of CUDA code to HIP, aiming to facilitate portability across both AMD and NVIDIA GPUs.</span><span class="c15">27</span><span class="c6">&nbsp;This tool is a critical component in AMD&#39;s strategy to attract researchers with existing CUDA codebases.</span></p><p class="c3"><span class="c8">A recurring pattern in researcher feedback suggests a notable discrepancy between AMD&#39;s official statements of framework support and the practical reality experienced by users. While high-level compatibility with major frameworks is indeed present, a deeper examination of user experiences reveals significant practical challenges. These include limitations with specific GPU models (e.g., the AMD Ryzen AI 300 Series not being fully supported by ROCm, or RDNA2 cards losing support in newer ROCm versions), persistent compilation issues, and unexpected performance regressions.</span><span class="c15">18</span><span class="c6">&nbsp;This gap between advertised compatibility and real-world usability creates considerable frustration for researchers, particularly those operating with consumer-grade hardware or non-standard system configurations. This perceived lack of consistent, out-of-the-box functionality undermines confidence in AMD&#39;s software stack and contributes to the perception that it is less mature and reliable than NVIDIA&#39;s, even when the high-level frameworks are technically &quot;supported.&quot; This represents a significant barrier to broader adoption beyond well-funded HPC centers that possess dedicated IT support and resources for extensive troubleshooting.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Performance Benchmarks and Scalability in Cluster/Supercomputer Environments</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD&#39;s Instinct MI300 series, encompassing MI300A, MI300X, MI350X, and MI355X, exhibits leading performance for both AI and HPC workloads. The MI350 Series, in particular, delivers up to a 4x generation-on-generation improvement in AI compute and a remarkable 35x leap in inferencing performance compared to previous ROCm 6 releases.</span><span class="c15">6</span><span class="c8">&nbsp;These accelerators are characterized by their high memory capacity (288GB HBM3E) and substantial memory bandwidth (up to 8TB/s), which are critical for processing large models and datasets in both inference and training tasks.</span><span class="c12">7</span></p><p class="c3"><span class="c8">AMD-powered supercomputers, such as Frontier and El Capitan, have successfully breached the exascale barrier, demonstrating the formidable scalability of AMD Instinct accelerators and the ROCm software stack across tens of thousands of GPUs.</span><span class="c15">2</span><span class="c6">&nbsp;These deployments validate AMD&#39;s capability to deliver performance at the highest echelons of global computing.</span></p><p class="c3"><span class="c8">In competitive benchmarking, the AMD MI300/MI325 series can, in certain workloads, surpass NVIDIA&#39;s H100. This is particularly evident in scenarios like Llama3 70B FP16 at ultra-low latency and Llama3 405B FP8 for chat, translation, and reasoning tasks at low latency, often attributed to AMD&#39;s superior HBM capacity.</span><span class="c15">28</span><span class="c8">&nbsp;However, NVIDIA&#39;s B200 with TensorRT LLM frequently demonstrates dominance for LLaMA 70B and 405B inference, and the H200 can be more competitive at longer latencies for certain tasks.</span><span class="c12">28</span></p><p class="c3"><span class="c8">AMD is actively enhancing its distributed inference capabilities with ROCm 7, leveraging frameworks such as SGLang and vLLM.</span><span class="c15">6</span><span class="c8">&nbsp;Furthermore, Triton-Distributed, a novel extension of OpenAI&#39;s Triton framework for AMD GPUs, aims to optimize performance by overlapping computation with communication, showing potential for a 30-40% improvement over PyTorch+RCCL in specific cases.</span><span class="c12">86</span></p><p class="c3"><span class="c8">Despite AMD&#39;s impressive raw hardware performance and scalability at the high end, particularly in exascale systems and with the MI350/MI400 series, there is a consistent observation that this power does not always translate into a clear cost-per-performance advantage, especially in rented cloud environments or for highly complex AI workloads. This apparent paradox is closely tied to the maturity of the software ecosystem and the comprehensiveness of continuous integration (CI) coverage.</span><span class="c15">28</span><span class="c8">&nbsp;While AMD&#39;s hardware can deliver competitive or even superior raw metrics in certain areas, NVIDIA&#39;s long-standing software ecosystem, extensive CI/CD processes, and mature inference frameworks (e.g., TensorRT LLM) enable it to extract more consistent and often more cost-effective performance from its hardware. This highlights that the &quot;software moat&quot; is not merely about the ability to port code, but about the overall efficiency and reliability of the entire software stack in enabling optimal hardware utilization.</span><span class="c15">15</span><span class="c6">&nbsp;AMD&#39;s stated commitment to a &quot;developer first&quot; approach and improving CI processes directly addresses this challenge, acknowledging that raw hardware power alone is insufficient for widespread adoption.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Experiences with Multi-GPU and Distributed Computing</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">ROCm facilitates multi-GPU and distributed computing through technologies such as AMD Infinity Fabric, which provides high-bandwidth inter-GPU communication, crucial for scaling workloads across multiple accelerators.</span><span class="c15">3</span><span class="c8">&nbsp;The ROCm ecosystem also integrates with leading orchestration platforms like Kubernetes and Slurm, enabling efficient management and deployment of large-scale GPU clusters.</span><span class="c12">9</span></p><p class="c3"><span class="c8">However, users have reported specific issues in multi-GPU systems, most notably applications hanging with 100% GPU utilization but without the expected temperature increase. This problem is frequently linked to the absence of the iommu=pt kernel parameter, indicating a configuration-related challenge in enabling stable multi-GPU operation.</span><span class="c15">22</span><span class="c8">&nbsp;Furthermore, discussions among HPC professionals highlight the inherent complexity of optimizing code for cross-accelerator designs and parallel programming paradigms prevalent in HPC environments.</span><span class="c12">32</span></p><p class="c3"><span class="c8">While AMD emphasizes the high-speed inter-GPU communication capabilities of Infinity Fabric, research suggests that increasing intra-node bandwidth and the number of accelerators can, paradoxically, sometimes impede overall inter-node communication performance due to congestion.</span><span class="c15">89</span><span class="c6">&nbsp;This observation implies that although the underlying hardware interconnect is powerful, the software stack and application-level optimizations for distributed workloads are still evolving to fully exploit this potential without inadvertently creating new bottlenecks. Achieving true scalability in multi-GPU and distributed environments necessitates more than just high-bandwidth interconnects. It demands sophisticated software components, such as optimized ROCm communication libraries and efficient MPI implementations, alongside robust profiling tools and a deep architectural understanding from developers. These elements are essential for effectively managing data movement and computation overlap. The current state suggests that researchers and HPC center staff might face a higher burden in fine-tuning applications for optimal multi-GPU performance on AMD platforms, compared to a potentially more &quot;out-of-the-box&quot; experience offered by NVIDIA&#39;s more mature ecosystem.</span></p><p class="c3"><span class="c9 c17">Table: Comparative Performance Benchmarks (AMD Instinct vs. NVIDIA in HPC/AI Workloads)</span></p><table class="c30"><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Feature/Metric</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">AMD Instinct MI300X/MI350X/MI355X</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">NVIDIA H100/H200/B200</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Architecture</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">CDNA 3/4 </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Hopper/Blackwell</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">FP64 Performance (TFLOPS)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI355X: 78.6 TFLOPS (GPU), 628.8 TFLOPS (8x Platform) </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">H100: 67 TFLOPS (GPU) </span><span class="c12">90</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">FP16 Performance (PFLOPS)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI355X: 5 PFLOPS (GPU), 40.2 PFLOPS (8x Platform) </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">H100: 312 TFLOPS (GPU) </span><span class="c12">91</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">FP8 Performance (PFLOPS)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI355X: 10.1 PFLOPS (GPU), 80.5 PFLOPS (8x Platform) </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">B200: Dominates LLaMA 70B/405B benchmarks </span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">FP4 Performance (PFLOPS)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI355X: 20.1 PFLOPS (GPU), 161 PFLOPS (8x Platform) </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">B200: Dominates LLaMA 70B/405B benchmarks </span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Memory Capacity (HBM)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI350X/MI355X: 288GB HBM3E </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">H100: 80GB HBM3; H200: 144GB HBM3E </span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Memory Bandwidth</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI350X/MI355X: 8 TB/s </span><span class="c12">7</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">H100: 3.35 TB/s; H200: 4.8 TB/s </span><span class="c12">90</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">LLM Inference (e.g., Llama3 70B FP16, 405B FP8)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">MI325X/MI300X can surpass H100 in perf/$ at ultra-low latency; MI325X competitive with H200 at higher latencies for some models.</span><span class="c12">28</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">B200 with TRT-LLM dominates; H200 with vLLM/TRT-LLM competitive at mid-high latencies.</span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">LLM Training</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">ROCm 7 delivers 3x training performance uplift over ROCm 6.</span><span class="c12">6</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Strong performance, established ecosystem.</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Cost-Effectiveness (Owned Clusters)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">Varies by workload; stronger perf/$ in some cases (e.g., Llama3 70B FP16 at ultra-low latency, Llama3 405B FP8 chat/reasoning at low latency).</span><span class="c12">28</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">Varies by workload; stronger perf/$ in others (e.g., B200 dominates).</span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Cost-Effectiveness (GPU Rentals)</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">Limited availability, elevated rental prices, less competitive perf/$.</span><span class="c12">28</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">Broad availability, price competition, generally better perf/$.</span><span class="c12">28</span></p></td></tr><tr class="c7"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c0">Software Ecosystem Maturity</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">ROCm rapidly evolving, but CI coverage &lt;10% of NVIDIA; complex config flags; accuracy issues observed.</span><span class="c12">28</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c10">CUDA mature, extensive tools, robust CI coverage; TRT-LLM highly performant.</span><span class="c12">28</span></p></td></tr></table><p class="c26 c14"><span class="c12"></span></p><h2 class="c25"><span class="c13">Software Stack Stability, Documentation, and Support</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Driver and Software Stack Stability for Research Workloads</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">User feedback frequently highlights concerns regarding the stability of AMD&#39;s drivers and software stack, particularly when using consumer-grade GPUs. Reports describe issues ranging from display problems and system hangs following ROCm installation to instances where ROCm completely ceases to function after specific version updates.</span><span class="c15">15</span><span class="c6">&nbsp;More granular issues include crashes of the</span></p><p class="c3"><span class="c8">amdgpu kernel driver </span><span class="c15">76</span><span class="c6">, performance regressions observed with new ROCm versions (e.g., ROCm 6.4 impacting</span></p><p class="c3"><span class="c8">llama.cpp performance </span><span class="c15">76</span><span class="c8">), and problematic VRAM usage spikes on Linux systems.</span><span class="c12">21</span></p><p class="c3"><span class="c8">AMD acknowledges these challenges and is actively working to enhance the Linux experience, with a stated goal of achieving a more seamless out-of-the-box experience and day-one client support for new hardware in the second half of 2025.</span><span class="c15">93</span><span class="c8">&nbsp;Recent ROCm releases, such as ROCm 6.4, have introduced features designed to improve stability and manageability in data center environments, including automatic GPU scheduling and driver management for Kubernetes clusters, as well as the ability to perform independent driver and ROCm updates.</span><span class="c12">9</span></p><p class="c3"><span class="c8">The observed divergence in stability experiences suggests a bifurcated development and quality assurance strategy within AMD. While significant investments and improvements are evident in the stability and management tools for </span><span class="c33">data center</span><span class="c8">&nbsp;environments, particularly for Instinct GPUs, user feedback consistently points to lingering instability and driver issues on </span><span class="c33">consumer</span><span class="c8">&nbsp;GPUs. This pattern indicates that the development focus is heavily weighted towards high-margin data center products. This creates a substantial barrier for early-career researchers, students, and smaller laboratories, who typically rely on more affordable consumer hardware for their initial exploration and development. If the entry-level experience with AMD GPUs is fraught with difficulties, it impedes the organic growth of a broader ROCm developer community, thereby perpetuating NVIDIA&#39;s long-standing ecosystem advantage.</span><span class="c15">21</span><span class="c8">&nbsp;AMD&#39;s recent public commitments to &quot;day-one client support&quot; and expanding support for more consumer cards indicate an awareness of this critical gap and a potential shift in resource allocation to address it.</span><span class="c12">26</span></p><p class="c3 c14"><span class="c12"></span></p><h3 class="c25"><span class="c19 c17">Documentation Quality and Gaps for Advanced/Research Use Cases</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD provides a comprehensive suite of documentation, developer hubs, and resources for ROCm, including detailed installation guides, API references, and performance tuning guides.</span><span class="c15">11</span><span class="c8">&nbsp;The company also publishes dedicated blogs and a &quot;ROCm Revisited&quot; series, explicitly aimed at improving developer enablement and sharing the evolution of the ROCm platform.</span><span class="c12">6</span></p><p class="c3"><span class="c8">Despite these extensive efforts, user feedback and academic reports frequently point to significant gaps in documentation, particularly for advanced or less common research use cases. For instance, the official TensorFlow documentation does not explicitly cover ROCm support, requiring users to consult AMD&#39;s own documentation for installation instructions.</span><span class="c15">51</span><span class="c8">&nbsp;Some users describe the ROCm installation instructions as &quot;obtuse,&quot; indicating a lack of clarity or user-friendliness.</span><span class="c15">18</span><span class="c8">&nbsp;A research paper critically notes that &quot;shortcomings encompass documentation, tooling, stability, compatibility, and correctness&quot; within the ROCm software stack, suggesting systemic issues beyond mere omissions.</span><span class="c15">25</span><span class="c8">&nbsp;Another study highlights user confusion regarding framework usage, configuration, and supported functionalities, citing &quot;unclear installation guidance&quot; and &quot;documentation gaps&quot; for advanced topics like multi-GPU setups or mixed-precision support.</span><span class="c12">82</span></p><p class="c3"><span class="c8">This pattern suggests that while the </span><span class="c33">breadth</span><span class="c8">&nbsp;of topics covered by AMD&#39;s documentation may be extensive, the </span><span class="c33">depth</span><span class="c8">&nbsp;and </span><span class="c33">clarity</span><span class="c8">&nbsp;required for complex, real-world research scenarios are often insufficient. The challenge is not necessarily a complete absence of documentation, but rather that existing resources are often inadequate for troubleshooting non-standard configurations, optimizing advanced workloads, or providing clear guidance on intricate technical details. This situation compels researchers to spend considerable time on debugging and seeking community-driven workarounds, which significantly reduces their productivity and increases the perceived difficulty of utilizing AMD GPUs.</span><span class="c15">18</span><span class="c8">&nbsp;For a platform aspiring to leadership in HPC, comprehensive and easily discoverable advanced documentation is as vital as basic installation guides. The observed lack of detailed, transparent continuous integration/continuous delivery (CI/CD) information further compounds this issue, as users cannot readily verify expected behavior or diagnose performance anomalies.</span><span class="c12">21</span></p><p class="c3 c14"><span class="c12"></span></p><h3 class="c25"><span class="c19 c17">Effectiveness of Support Channels and Community Engagement</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD has demonstrated an increasing commitment to engaging with its developer community. The company actively encourages community contributions to ROCm-DS and other components, welcoming bug reports, fixes, and documentation additions through its GitHub Issues and Discussions platforms.</span><span class="c15">97</span><span class="c8">&nbsp;AMD also operates structured programs such as the AMD University Program and offers cloud access initiatives designed to support developers and academic institutions.</span><span class="c12">34</span></p><p class="c3"><span class="c8">A notable positive development is the direct engagement from AMD&#39;s leadership. Anush Elangovan, AMD&#39;s AI Software Czar, has actively participated in community discussions, openly acknowledging gaps in the ROCm software stack and expressing a strong desire for improvement. This engagement has extended to adopting community-contributed code for benchmarks, signaling a responsive and collaborative approach.</span><span class="c12">21</span></p><p class="c3"><span class="c8">However, despite these positive shifts, some users continue to express concerns. There is a persistent sentiment that AMD needs to allocate more resources to developing robust Windows support for ROCm and to more actively contribute to third-party machine learning projects.</span><span class="c15">21</span><span class="c8">&nbsp;The limited availability of easily accessible cloud hardware for testing purposes also remains a concern for many researchers, hindering their ability to experiment and develop on AMD platforms.</span><span class="c12">21</span></p><p class="c3"><span class="c8">This pattern suggests that while AMD&#39;s </span><span class="c33">official engagement</span><span class="c8">&nbsp;with the research community is improving, as evidenced by direct executive involvement and structured programs, the </span><span class="c33">execution and resource allocation</span><span class="c6">&nbsp;required to address widespread, granular issues&mdash;particularly for consumer hardware and Windows environments&mdash;have not yet fully met user expectations. This implies that while AMD is successfully building goodwill and laying a foundation for collaboration, the sheer volume and complexity of the software ecosystem challenges necessitate sustained and significantly increased investment in developer-facing resources. This includes dedicated engineering efforts for less &quot;strategic&quot; but high-volume consumer platforms. For AMD&#39;s &quot;developer first&quot; strategy to be truly effective, it must translate into a consistently smooth and reliable experience across all relevant hardware tiers, not just the flagship data center deployments.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Institutional and Procurement Challenges</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Experiences with Grant-Funded Hardware Purchases</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD has established specific programs to facilitate the acquisition of its hardware by academic institutions. The &quot;AI &amp; HPC Cluster&quot; program is tailored for academic institutions and non-profit research organizations seeking computing resources for research and education.</span><span class="c15">34</span><span class="c8">&nbsp;Additionally, the AMD University Program (AUP) includes a &quot;Donation Program&quot; that provides hardware and software licenses for teaching and research purposes.</span><span class="c15">35</span><span class="c6">&nbsp;These initiatives aim to lower the financial and logistical barriers for academic access to high-performance computing.</span></p><p class="c3"><span class="c8">Case studies illustrate successful grant-funded deployments facilitated by these programs. Chile&#39;s National Laboratory for High Performance Computing (NLHPC), for instance, successfully deployed Lenovo servers equipped with 4th Gen AMD EPYC CPUs and AMD Instinct MI210 GPUs. This implementation resulted in significant performance and energy efficiency gains, particularly for molecular dynamics workloads.</span><span class="c15">36</span><span class="c8">&nbsp;Similarly, the University of Tartu achieved HPC excellence by migrating to Lenovo servers powered by AMD EPYC processors, highlighting the cost-effectiveness of their on-premises HPC cluster compared to cloud hosting providers.</span><span class="c12">37</span></p><p class="c3"><span class="c8">A closer examination of these successful deployments often reveals a common element: direct collaboration with AMD engineers. The NLHPC case study explicitly mentions that the AMD team proactively engaged with NLHPC, connecting them with US engineers to ensure a &quot;smooth transition&quot; and to help them achieve the &quot;best solution&quot; during the integration of AMD GPUs.</span><span class="c15">36</span><span class="c6">&nbsp;This suggests that while hardware access might be facilitated through donation or dedicated programs, the subsequent integration and optimization for production-level research often necessitate substantial vendor-side support. This level of direct engineering assistance may not be readily available to smaller, less-resourced institutions. The observation indicates that the success of grant-funded purchases is highly contingent upon the extent of direct engineering support provided by AMD. To scale adoption beyond a select few flagship supercomputing centers, AMD would benefit from productizing this &quot;integration support&quot; through more robust, self-service tools, comprehensive documentation, and a more stable software stack. Such advancements would significantly reduce the burden on institutional IT staff and researchers themselves, aligning with the broader objective of improving the &quot;out-of-the-box&quot; experience.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Challenges with Institutional IT Support and Integration</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">HPC centers and academic institutions face a myriad of challenges when deploying new computing platforms, particularly those involving heterogeneous architectures. These challenges include adapting existing legacy infrastructure, which often struggles with the increased power and cooling demands of modern processors and accelerators, leading to potential bottlenecks if not properly retrofitted.</span><span class="c15">32</span><span class="c8">&nbsp;The integration of multiple processors and accelerators introduces significant system complexity, making it difficult to accurately forecast workloads and optimize code for cross-accelerator designs, a task that requires advanced parallel programming skills.</span><span class="c15">32</span><span class="c8">&nbsp;Ensuring consistency in workloads across hybrid on-premises and cloud environments also presents a challenge, as results must remain reliable regardless of where the computation occurs.</span><span class="c15">32</span><span class="c8">&nbsp;Furthermore, the design of an effective HPC architecture demands sophisticated networking and storage infrastructure to manage the enormous amounts of data generated and processed.</span><span class="c15">32</span><span class="c8">&nbsp;The specialized expertise required for hardware abstraction and optimizing code for these complex, heterogeneous environments is frequently lacking in-house within many institutions.</span><span class="c12">32</span></p><p class="c3"><span class="c8">AMD is actively developing solutions to mitigate these challenges for IT staff. The AMD GPU Operator, for instance, aims to simplify cluster management by providing automatic GPU scheduling and driver management for Kubernetes clusters, with support for Red Hat OpenShift and Ubuntu.</span><span class="c15">9</span><span class="c6">&nbsp;This tool is designed to minimize manual configuration efforts and streamline workload deployment.</span></p><p class="c3"><span class="c6">The complexities inherent in HPC deployments are undeniable. However, the specific challenges related to &quot;incompatible components,&quot; the difficulty of &quot;optimizing code for complex environments,&quot; and the &quot;lack of in-house expertise&quot; directly point to a significant operational burden placed on institutional IT staff. If AMD&#39;s software stack necessitates more manual intervention, custom compilation, or extensive troubleshooting compared to NVIDIA&#39;s more established ecosystem, it inherently increases the operational overhead for IT teams. This makes AMD a less attractive option for institutions, even if its hardware offers potential performance advantages. For AMD to increase its adoption in academic and research institutions, it must actively strive to reduce this &quot;IT staff burden.&quot; This entails providing more robust, enterprise-grade deployment tools, such as the GPU Operator, alongside comprehensive and actionable troubleshooting guides. Ensuring backward and forward compatibility across software releases is also critical to minimize the effort required for recompilation and ongoing maintenance. The ultimate goal should be to make AMD GPU integration as seamless and low-effort as possible for institutional IT departments.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Procurement Difficulties and Market Dynamics for Academic Institutions</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">The data center GPU market is currently experiencing substantial growth, driven by the escalating demand for artificial intelligence and deep learning workloads. However, this market is highly concentrated, with NVIDIA and AMD holding significant sway.</span><span class="c15">100</span><span class="c8">&nbsp;NVIDIA, in particular, with its comprehensive full-stack approach (covering GPU, CPU, and DPU technologies), presents a formidable challenge. The high switching costs associated with moving away from NVIDIA&#39;s platform, deeply rooted in the complexities of its integrated ecosystem, make it difficult for GPU server manufacturers and, by extension, academic institutions, to consider alternatives.</span><span class="c12">100</span></p><p class="c3"><span class="c8">Compounding these market dynamics are broader supply chain issues. The AI chip industry is facing ongoing shortages and bottlenecks, particularly for High Bandwidth Memory (HBM3), a critical component for high-end GPUs from both AMD and NVIDIA. These shortages are leading to elevated prices and extended lead times, impacting procurement for all institutions, regardless of their preferred vendor.</span><span class="c12">101</span></p><p class="c3"><span class="c8">For academic researchers and smaller laboratories, who often rely on flexible access to computing resources, the GPU rental market plays a crucial role. In this segment, NVIDIA consistently offers better performance per dollar for short-to-medium term contracts (under six months). This cost-effectiveness is driven by the broad availability of NVIDIA GPUs from over 100 different Neocloud providers, fostering intense price competition. In stark contrast, AMD GPU rentals are limited in availability, leading to an artificially tight market and consequently higher prices for AMD GPU capacity.</span><span class="c12">28</span></p><p class="c3"><span class="c8">The limited competitive options for AMD GPU rentals acts as a significant bottleneck for grassroots adoption and initial experimentation. If researchers cannot easily and affordably &quot;try before they buy&quot; on AMD platforms, or if the rental experience is inferior in terms of cost or accessibility, it naturally reinforces the existing CUDA ecosystem&#39;s lock-in.</span><span class="c15">34</span><span class="c8">&nbsp;AMD&#39;s Developer Cloud represents a direct attempt to address this issue by providing direct access to AMD Instinct GPUs.</span><span class="c15">6</span><span class="c6">&nbsp;However, the scale of this offering and its ability to compete on pricing within the broader cloud rental market will be crucial factors in influencing future procurement decisions and fostering wider adoption.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Barriers to Porting Existing CUDA/NVIDIA-based Research Code</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c17 c19">Technical Hurdles and HIP Conversion Experiences</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">The transition from CUDA to ROCm presents a non-trivial undertaking for researchers, despite the design of HIP (Heterogeneous-Compute Interface for Portability) to ease this conversion process.</span><span class="c15">27</span><span class="c8">&nbsp;HIP aims to enable automatic translation of CUDA programs and allow HIP programs to run natively on both AMD (via ROCm) and NVIDIA (via CUDA) GPUs.</span><span class="c12">73</span></p><p class="c3"><span class="c8">However, the reality of porting reveals that &quot;small differences&quot; between HIP C++ and CUDA C++ frequently necessitate manual intervention to adjust existing codebases.</span><span class="c15">31</span><span class="c8">&nbsp;This is particularly true because specific CUDA features or libraries may lack direct equivalents within the ROCm ecosystem, leading to functional gaps that must be addressed by developers.</span><span class="c15">27</span><span class="c6">&nbsp;Users have reported difficulties with</span></p><p class="c3"><span class="c8">hipify tools, and issues can arise from hard-coded dependencies on warp size or the use of specific CUDA intrinsics that do not translate seamlessly.</span><span class="c15">104</span><span class="c6">&nbsp;During the porting process, researchers may encounter various error codes such as</span></p><p class="c3"><span class="c8">hipErrorNotSupported, hipErrorInvalidDeviceFunction, or hipErrorLaunchFailure, all of which indicate underlying compatibility or resource allocation issues that require in-depth debugging.</span><span class="c12">105</span></p><p class="c3"><span class="c6">The observation that hipify provides a good starting point for code translation, but often requires significant manual intervention for &quot;small differences&quot; or &quot;functionality gaps,&quot; suggests that automated tools only cover a portion of the migration effort. The most complex and performance-critical segments of scientific code, which frequently rely on highly optimized CUDA kernels or unique library features, are precisely where the &quot;last mile&quot; of porting becomes a substantial technical burden. This situation implies that for many established research groups with extensive, highly optimized CUDA codebases, the perceived effort and inherent risks associated with porting to ROCm often outweigh the potential benefits, especially if their existing NVIDIA infrastructure adequately meets their needs. For AMD to overcome this, it must either significantly enhance the intelligence and scope of the hipify tool to handle more complex cases automatically or provide more detailed, workload-specific porting guides and highly optimized ROCm equivalents for common CUDA programming patterns.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Developer Learning Curve and Ecosystem Gaps</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">The significant &quot;switching costs&quot; associated with moving away from a specific GPU manufacturer&#39;s platform are deeply rooted in the complexities of the integrated hardware and software ecosystem, particularly NVIDIA&#39;s.</span><span class="c15">100</span><span class="c8">&nbsp;NVIDIA&#39;s CUDA has long been considered the &quot;gold standard&quot; and enjoys widespread support across deep learning frameworks, contributing to its entrenched position.</span><span class="c12">15</span></p><p class="c3"><span class="c8">In contrast, users frequently describe ROCm as &quot;difficult to use and install,&quot; citing &quot;a ton of compatibility issues&quot; and the challenge of navigating &quot;so many versions of all the needed apps&quot; that it becomes &quot;virtually impossible&quot; to achieve a functional setup without extensive troubleshooting.</span><span class="c15">23</span><span class="c8">&nbsp;A critical concern is the perceived lack of robust and transparent continuous integration/continuous delivery (CI/CD) for popular open-source projects on AMD hardware. This deficiency often leads to a higher incidence of bugs and significantly hinders the overall developer experience, contrasting sharply with NVIDIA&#39;s more mature and widespread CI coverage.</span><span class="c12">21</span></p><p class="c3"><span class="c8">The challenges in porting code extend beyond mere syntax; they encompass the entire developer experience. NVIDIA&#39;s &quot;CUDA dominance&quot; is built upon years of mature tooling, comprehensive libraries, extensive community resources, and a consistent user experience across a wide range of hardware.</span><span class="c15">29</span><span class="c8">&nbsp;AMD&#39;s ROCm, despite its open-source nature, continues to grapple with fragmentation, inconsistent support for consumer cards, and a perceived lack of polish in its tooling and overall user experience.</span><span class="c15">21</span><span class="c8">&nbsp;This creates a powerful &quot;ecosystem lock-in&quot; for NVIDIA. Researchers are less inclined to transition if it means facing a steep learning curve, unreliable tools, and a higher probability of encountering unresolved issues. AMD&#39;s &quot;developer first&quot; strategy and its efforts to align HIP C++ more closely with CUDA are direct responses to this challenge.</span><span class="c15">8</span><span class="c8">&nbsp;However, consistent execution and visible improvements in everyday usability are paramount. The recurring suggestion from the community to &quot;make installation of the runtime libraries and extensions as easy as the CUDA libs through PyPI&quot; highlights a key user desire for simplified dependency management, which would significantly reduce the friction in adopting ROCm.</span><span class="c12">21</span></p><p class="c3 c14"><span class="c12"></span></p><h3 class="c25"><span class="c19 c17">Community Feedback on Porting Efforts and Desired Improvements</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">Community feedback consistently highlights a strong desire for improvements that would democratize access to AMD&#39;s GPU computing capabilities. Researchers express a clear need for broader support for consumer Radeon discrete GPUs (dGPUs) and integrated GPUs (iGPUs) within ROCm, alongside a plea to prevent the deprecation of cards that possess sufficient VRAM.</span><span class="c15">21</span><span class="c8">&nbsp;There is also a significant demand for full native Windows support for ROCm.</span><span class="c12">14</span></p><p class="c3"><span class="c8">A frequent request centers on enhancing the user-friendliness of the installation process and overall user experience. Researchers advocate for a simplified installation of runtime libraries and extensions, ideally as straightforward as installing CUDA libraries via PyPI (e.g., a simple pip install rocm-runtime).</span><span class="c15">21</span><span class="c8">&nbsp;Furthermore, community members explicitly call for &quot;robust, transparent CI for these popular projects, running frequently across all cards,&quot; emphasizing the need for open and accessible CI results.</span><span class="c15">21</span><span class="c8">&nbsp;They also urge AMD to more actively contribute to third-party machine learning projects (e.g., vLLM, bitsandbytes, unsloth) to ensure seamless operation and integration.</span><span class="c12">21</span></p><p class="c3"><span class="c8">A prevalent sentiment articulated by researchers is that NVIDIA&#39;s ecosystem thrives because &quot;every student with a standard computer and an NVIDIA card can easily run their small projects,&quot; a level of accessibility often not matched by AMD.</span><span class="c15">21</span><span class="c6">&nbsp;This observation underscores a fundamental challenge for AMD: while securing high-end HPC wins is crucial for market share and validation, neglecting the grassroots developer community, particularly those reliant on consumer hardware, limits the organic growth of the ROCm ecosystem. To genuinely bridge the &quot;CUDA moat,&quot; AMD must significantly invest in simplifying the developer experience across all hardware tiers. This means making it as effortless as possible for new users to get started, experiment, and contribute, which includes providing better support for Windows and a more streamlined installation process. The community&#39;s suggestions offer a clear roadmap for achieving this democratization of HPC access.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Researcher Sentiment and Pain Point Analysis</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Overall Sentiment Categorization (Positive, Neutral, Negative)</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c6">The collective sentiment among researchers regarding AMD GPU software and hardware presents a bifurcated landscape, reflecting a divide between high-end, well-resourced deployments and the experiences of individual users with more modest setups.</span></p><p class="c3"><span class="c9">Positive Sentiment:</span><span class="c8">&nbsp;This outlook is predominantly observed in the context of large-scale HPC deployments and official communications from AMD. The primary drivers of this positive sentiment include AMD&#39;s undeniable success in powering exascale systems like Frontier and El Capitan, which consistently demonstrate leadership in performance and energy efficiency.</span><span class="c15">1</span><span class="c8">&nbsp;The continuous evolution of the ROCm software stack, marked by significant performance boosts and expanded framework support, also contributes to this positive perception, particularly among those with access to high-end Instinct hardware and dedicated support teams.</span><span class="c15">6</span><span class="c8">&nbsp;Academic collaborations and specialized workshops, such as the University of Oregon&#39;s hackathon with MI300A APUs, further reinforce positive experiences for select research groups.</span><span class="c12">41</span></p><p class="c3"><span class="c9">Negative Sentiment:</span><span class="c8">&nbsp;This perspective is pervasive among individual researchers, users of consumer-grade GPUs, and those attempting to port existing CUDA code without extensive institutional backing. The negative sentiment is fueled by persistent and widely reported issues, including complex and unstable installation processes, recurring driver problems, limited official support for consumer-grade GPUs, and the perceived difficulty of navigating the &quot;CUDA moat&quot; to achieve functional parity.</span><span class="c15">15</span><span class="c8">&nbsp;The scarcity of competitive cloud rental options for AMD GPUs also contributes to negative sentiment regarding accessibility and cost-effectiveness for smaller, exploratory projects.</span><span class="c12">28</span></p><p class="c3"><span class="c9">Neutral Sentiment:</span><span class="c8">&nbsp;This stance is less explicitly stated but can be inferred from discussions where users acknowledge AMD&#39;s ongoing efforts and the potential of its technology while simultaneously grappling with practical implementation challenges. Some researchers express a desire for AMD to succeed in challenging NVIDIA&#39;s monopoly but find the current state of ROCm too demanding for their immediate needs.</span><span class="c15">21</span><span class="c6">&nbsp;This sentiment is often characterized by a &quot;wait and see&quot; approach or a recognition that while improvements are occurring, the platform is &quot;not there yet&quot; for their specific use cases.</span></p><p class="c3"><span class="c8">The dual nature of sentiment&mdash;highly positive at the top-tier HPC level and notably negative at the individual researcher level&mdash;reveals a fundamental disconnect between AMD&#39;s strategic successes in high-profile computing and the practical, day-to-day experience of a broader research community. AMD has unequivocally demonstrated its </span><span class="c33">potential</span><span class="c8">&nbsp;and </span><span class="c33">capability</span><span class="c8">&nbsp;at scale, but it has not yet consistently translated this into widespread </span><span class="c33">practicality</span><span class="c8">&nbsp;and </span><span class="c33">ease of use</span><span class="c6">&nbsp;for the average researcher. This situation implies that AMD&#39;s current strategy, while securing major market validation, risks alienating a large segment of the research community that could otherwise become future advocates and contributors to the ROCm ecosystem. Bridging this gap necessitates a deliberate reallocation of resources and a renewed development focus to address the &quot;long tail&quot; of user pain points, particularly those related to consumer hardware and simplified software deployment.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Table: Ranked Pain Points and Frequencies (with specific research examples and quotes)</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><table class="c30"><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Rank</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Pain Point</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Description &amp; Specific Examples</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Illustrative Quotes/References</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">1</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Installation &amp; Setup Difficulties</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Complex, unstable, and non-intuitive installation processes, especially for consumer GPUs and non-officially supported Linux distros. Issues with secure boot, iGPU conflicts, and dependency hell.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;I have been trying for literally the last 12 hours to get this to work...&quot;.</span><span class="c15">19</span><span class="c10">&nbsp;&quot;Definitely need secure boot off.&quot;.</span><span class="c15">19</span><span class="c10">&nbsp;&quot;I would recommend disabling your iGPU in the BIOS...&quot;.</span><span class="c15">19</span><span class="c10">&nbsp;&quot;The installer is so bad, and has real issues with kernel versions and a bunch of other crap.&quot;.</span><span class="c15">19</span><span class="c10">&nbsp;&quot;ROCm completely stopped working after version 6.14.0-202 on Linux.&quot;.</span><span class="c15">76</span><span class="c10">&nbsp;&quot;The official ROCm installation process from AMD&#39;s website wasn&#39;t well-suited for Pop!_OS, leading to dependency conflicts and version mismatches.&quot;.</span><span class="c12">24</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Limited Consumer GPU Support &amp; Fragmentation</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Official ROCm support is primarily for data center Instinct cards, with inconsistent or unofficial support for consumer Radeon GPUs (RDNA2/3). This forces users into workarounds or compilation from source.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;Functionally ROCm runs on all RDNA2/3 chips, it&#39;s just that they&#39;re not properly QA&#39;d so officially they can only say it runs for navi21, navi31 etc..&quot;.</span><span class="c15">21</span><span class="c10">&nbsp;&quot;So, if you were hoping to use your new AMD Ryzen AI 300 Series laptop with PyTorch, it&#39;s not going to work. AMD&#39;s marketing is being misleading here.&quot;.</span><span class="c15">74</span><span class="c10">&nbsp;&quot;It&#39;s almost impossible to experiment with it using simple, entry-level hardware - gamer gear.&quot;.</span><span class="c15">21</span><span class="c10">&nbsp;&quot;ROCM is increasingly dropping support of 5+ year old hardware.&quot;.</span><span class="c12">23</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">3</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Software Stack Stability &amp; Driver Issues</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Frequent driver crashes, unexpected system hangs, VRAM usage spikes, and performance regressions with new ROCm versions. Perceived as less polished than NVIDIA&#39;s software.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;I do have horrible VRAM usage spikes nowadays on last LTS Ubuntu version and last release of ROCm I did not have on 5.7 and some older LTS...&quot;.</span><span class="c15">21</span><span class="c10">&nbsp;&quot;[Issue]: amdgpu kernel driver crashes with standard workloads.&quot;.</span><span class="c15">76</span><span class="c10">&nbsp;&quot;[Issue]: ROCM 6.4 performance regression with llama.cpp.&quot;.</span><span class="c15">76</span><span class="c10">&nbsp;&quot;some users still report issues with stability and bugs compared to NVIDIA&#39;s more polished software.&quot;.</span><span class="c12">15</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">4</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">&quot;CUDA Moat&quot; &amp; Porting Difficulties</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Significant technical and ecosystem barriers to porting existing CUDA code. Lack of direct ROCm equivalents for some CUDA features/libraries, requiring manual intervention.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;Transitioning from CUDA to ROCm, however, is not a straightforward task.&quot;.</span><span class="c15">27</span><span class="c10">&nbsp;&quot;Small differences between our implementation of the HIP C++ programming model and CUDA C++ often require manual intervention...&quot;.</span><span class="c15">31</span><span class="c10">&nbsp;&quot;It&#39;s difficult to use and install and has a ton of compatibility issues.&quot;.</span><span class="c15">23</span><span class="c10">&nbsp;&quot;ROCm lacks the backwards compatibility CUDA has in many cases.&quot;.</span><span class="c12">23</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">5</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Documentation Gaps &amp; Lack of Transparency</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">While extensive, documentation lacks depth for advanced/non-standard use cases, troubleshooting, and optimization. Absence of transparent CI/CD results for popular frameworks.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;The official TensorFlow documentation does not cover ROCm support. Use the ROCm documentation for installation instructions...&quot;.</span><span class="c15">51</span><span class="c10">&nbsp;&quot;shortcomings encompass documentation, tooling, stability, compatibility, and correctness.&quot;.</span><span class="c15">25</span><span class="c10">&nbsp;&quot;Unclear Installation Guidance (A.1)... Documentation gaps (A.2.i) lead to confusion about core usage, such as multi-GPU setups...&quot;.</span><span class="c15">82</span><span class="c10">&nbsp;&quot;Robust, transparent CI for these popular projects, running frequently across all cards... The CI runs must be open.&quot;.</span><span class="c12">21</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">6</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Limited Cloud Availability &amp; High Rental Costs</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">Scarcity of AMD GPU instances in cloud rental markets compared to NVIDIA, leading to higher prices and hindering &quot;try before you buy&quot; opportunities for researchers.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;For customers that are using short to medium term rentals (sub-6 month) from Neoclouds, Nvidia always wins on performance per dollar.&quot;.</span><span class="c15">28</span><span class="c10">&nbsp;&quot;Harder to ask, but ask AMD to push cloud vendors to make the ROCm stack easy to test by having hardware available on all major platforms.&quot;.</span><span class="c12">21</span></p></td></tr><tr class="c7"><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">7</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c0">Ecosystem Maturity &amp; Third-Party Integration</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">NVIDIA&#39;s ecosystem is seen as more mature with broader, more consistent support for deep learning frameworks and libraries. AMD needs to contribute more actively to third-party ML projects.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c10">&quot;NVIDIA&#39;s CUDA... has become the gold standard for GPU-accelerated tasks...&quot;.</span><span class="c15">15</span><span class="c10">&nbsp;&quot;For most models, worse accuracy quality is observed on AMD compared to NVIDIA, with 25% of tested models failing accuracy tests on AMD.&quot;.</span><span class="c15">28</span><span class="c10">&nbsp;&quot;Contribute (more actively) to 3rd-party ML projects. I hope to run projects like VLLM, bitsandbytes, unsloth etc... without any issues on ALL cards.&quot;.</span><span class="c12">21</span></p></td></tr></table><p class="c26 c14"><span class="c12"></span></p><h3 class="c25"><span class="c19 c17">Researcher-Proposed Solutions and Improvements</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c25"><span class="c6">The research community has articulated several clear and actionable recommendations for AMD to enhance ROCm adoption and improve the user experience:</span></p><ol class="c21 lst-kix_list_1-0 start" start="1"><li class="c24 li-bullet-0"><span class="c9">Expand and Standardize Consumer GPU Support:</span><span class="c8">&nbsp;Researchers strongly advocate for AMD to officially support a wider range of consumer Radeon discrete GPUs and integrated GPUs with ROCm, ensuring consistent stability and performance across different generations. This includes a plea to avoid deprecating older cards that still possess sufficient VRAM.</span><span class="c15">21</span><span class="c6">&nbsp;The rationale is that consumer cards serve as the primary entry point for many early-career researchers and students, and fragmented or unofficial support compels users to resort to complex workarounds, thereby hindering adoption and reinforcing the perception of ROCm as unreliable.</span></li><li class="c29 li-bullet-0"><span class="c9">Implement User-Friendly Installation and Experience:</span><span class="c8">&nbsp;A top priority is to simplify the ROCm installation and setup process, particularly for consumer-grade GPUs and common Linux distributions. The community desires a &quot;one-click&quot; or pip install level of ease for core machine learning frameworks.</span><span class="c15">21</span><span class="c6">&nbsp;A smoother initial experience is considered crucial for cultivating a loyal user base and directly challenging NVIDIA&#39;s dominance at the grassroots level.</span></li><li class="c29 li-bullet-0"><span class="c9">Provide Full Native Windows Support:</span><span class="c8">&nbsp;There is a significant demand for comprehensive native Windows support for ROCm, enabling PyTorch and other frameworks to run efficiently without reliance on Windows Subsystem for Linux (WSL) or other workarounds.</span><span class="c15">14</span><span class="c6">&nbsp;This would broaden the accessibility of AMD GPUs for a large segment of the developer community.</span></li><li class="c29 li-bullet-0"><span class="c9">Enhance Documentation and Transparency:</span><span class="c8">&nbsp;Researchers call for improvements in the depth and clarity of documentation, especially for advanced use cases, troubleshooting common issues (e.g., multi-GPU hangs, performance regressions), and optimizing specific scientific workloads. A key suggestion is to implement transparent CI/CD results for popular frameworks across all supported hardware, making CI runs open and accessible.</span><span class="c15">21</span><span class="c6">&nbsp;While documentation exists, gaps in detail for complex scenarios force users into extensive self-debugging. Transparent CI/CD would build trust and allow users to understand expected performance and identify issues more quickly.</span></li><li class="c29 li-bullet-0"><span class="c9">Increase Investment in Software Ecosystem Maturity and Third-Party Contributions:</span><span class="c8">&nbsp;The community urges AMD to increase investment in internal R&amp;D cluster resources to improve CI coverage and overall software quality.</span><span class="c15">28</span><span class="c8">&nbsp;Furthermore, active contribution to and collaboration with third-party machine learning and HPC projects (e.g., vLLM, bitsandbytes, OpenMM, GROMACS, LAMMPS) is recommended to ensure robust, optimized, and out-of-the-box compatibility.</span><span class="c15">21</span><span class="c8">&nbsp;NVIDIA&#39;s &quot;software moat&quot; is built on a highly mature ecosystem, and AMD needs to close this gap by ensuring its software components are not only functional but also highly optimized, stable, and seamlessly integrated with the tools researchers already use. This includes addressing reported issues like &quot;dumber answers&quot; due to accuracy test gaps.</span><span class="c12">28</span></li><li class="c29 li-bullet-0"><span class="c9">Strategically Leverage Cloud and Academic Programs:</span><span class="c8">&nbsp;Expanding the AMD Developer Cloud offering to include a wider range of hardware (e.g., MI350 series, MI325X) and ensuring timely availability of the latest ROCm versions is crucial.</span><span class="c15">43</span><span class="c8">&nbsp;Additionally, AMD should encourage cloud vendors to offer more competitive AMD GPU rental options.</span><span class="c15">21</span><span class="c8">&nbsp;Cloud access serves as a critical &quot;try before you buy&quot; gateway for many researchers, and making AMD GPUs easily and affordably accessible in the cloud can significantly drive organic adoption and familiarization with the ROCm stack, influencing future procurement decisions. Continuing and expanding academic donation programs, while ensuring sufficient post-installation technical support, also remains valuable.</span><span class="c12">26</span></li></ol><p class="c28"><span class="c6">These proposed solutions collectively point towards a strong desire for ROCm to be more accessible and user-friendly, mirroring the ease of use often associated with NVIDIA&#39;s ecosystem for individual developers. This is not merely about technical fixes, but about democratizing access to GPU computing for a wider audience beyond large supercomputing centers. Implementing these suggestions, particularly around ease of installation, broader consumer GPU support, and transparent CI/CD, would significantly enhance ROCm&#39;s appeal to the broader academic and research community, fostering organic growth and reducing reliance on top-down institutional procurement.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Success Stories and Market Opportunity Assessment</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Documented Research Breakthroughs and Effective AMD GPU Deployments</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">AMD&#39;s presence in the high-performance computing landscape is marked by several significant success stories, particularly at the exascale level. The AMD-powered supercomputers Frontier and El Capitan are actively contributing to breakthroughs in science and engineering. The ROCm software stack plays a pivotal role in accelerating these discoveries through optimized compute kernels and mixed-precision AI workflows.</span><span class="c15">3</span><span class="c6">&nbsp;These systems demonstrate AMD&#39;s capability to deliver on the promise of exascale computing for complex scientific simulations and AI advancements.</span></p><p class="c3"><span class="c8">Beyond these flagship installations, various case studies highlight effective AMD GPU deployments in academic and research settings. The University of Oregon, for example, hosted a successful hackathon where researchers gained early access to MI300A APUs for diverse projects, including computational fluid dynamics, climate science, and AI. Participants reported success in porting their code and improving performance, demonstrating the practical utility of AMD&#39;s latest hardware.</span><span class="c15">41</span><span class="c8">&nbsp;Similarly, Chile&#39;s NLHPC achieved notable performance and energy efficiency gains in molecular dynamics simulations by deploying Lenovo servers equipped with 4th Gen AMD EPYC CPUs and AMD Instinct MI210 GPUs.</span><span class="c15">36</span><span class="c8">&nbsp;Goethe University in Europe is also leveraging large AMD GPU clusters for AI-driven physics and digital twin models, showcasing the increasing integration of AMD solutions in cutting-edge academic research.</span><span class="c12">42</span></p><p class="c3"><span class="c8">In the realm of AI, AMD Instinct GPUs are being utilized for large-scale AI model training, as evidenced by KT Cloud&#39;s use of MI250 accelerators and Lamini LLMs being built exclusively on Instinct accelerators.</span><span class="c15">91</span><span class="c8">&nbsp;Furthermore, AMD Instinct GPUs, leveraging vLLM and ROCm, are being used to deploy large language models such as Cohere&#39;s 104B-parameter Command R+ model for enterprise-grade inference.</span><span class="c12">6</span></p><p class="c3"><span class="c8">These concentrated successes at the high-performance computing and large-scale AI deployment levels serve as powerful validation points for AMD&#39;s hardware and software capabilities in the most demanding environments. These flagship deployments are critical for AMD&#39;s reputation and provide strong evidence of its technical prowess, which can be effectively leveraged in marketing and sales to attract other large-scale customers. However, the challenge remains in demonstrating how these successes translate into tangible benefits and ease of use for the </span><span class="c33">broader</span><span class="c6">&nbsp;academic and research community, particularly those without access to such massive resources or dedicated engineering support.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Unmet Needs and Potential Wins in Academia/HPC</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">Despite AMD&#39;s high-profile successes, a significant unmet need exists for accessible and user-friendly AMD GPU compute resources for individual researchers and smaller institutions, especially those relying on consumer-grade hardware.</span><span class="c15">21</span><span class="c6">&nbsp;This segment of the research community often struggles with the complexities of ROCm installation and the inconsistent support for non-data center GPUs.</span></p><p class="c3"><span class="c8">There is a palpable demand within the research community for a viable alternative to NVIDIA&#39;s dominant CUDA ecosystem. Researchers are actively seeking open-source, flexible solutions that offer freedom from vendor lock-in.</span><span class="c15">29</span><span class="c8">&nbsp;AMD&#39;s commitment to open standards and its open-source ROCm platform represent a strategic advantage in this regard, positioning it as a compelling choice for institutions and researchers who prioritize transparency and control over their software stack.</span><span class="c12">3</span></p><p class="c3"><span class="c8">The emerging AI PC market and edge AI deployments also present substantial opportunities for AMD. With products like the Ryzen AI 300 Series and the Radeon AI PRO R9700, AMD is actively targeting client-side AI workloads.</span><span class="c15">14</span><span class="c8">&nbsp;However, ensuring robust and consistent ROCm support for these client-side GPUs is a critical factor for capturing this market segment.</span><span class="c12">74</span></p><p class="c3"><span class="c6">AMD&#39;s primary market opportunity lies not simply in competing on raw performance or price, but in aggressively promoting and consistently delivering on the promise of a truly open, accessible, and robust ecosystem. This means prioritizing community contributions, ensuring seamless integration with popular open-source frameworks, and making it genuinely easy for developers to get started and scale their work on AMD hardware, irrespective of their budget or institutional affiliation. This strategic direction, if executed effectively, could significantly expand AMD&#39;s footprint in the academic and HPC sectors.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Competitive Analysis: NVIDIA/CUDA vs. AMD/ROCm in Research</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c8">NVIDIA currently maintains a substantial lead in the AI/machine learning and scientific computing domains, largely due to its mature CUDA framework, extensive support across deep learning frameworks (including TensorFlow and PyTorch), and a deeply entrenched developer ecosystem.</span><span class="c15">15</span><span class="c8">&nbsp;NVIDIA&#39;s H100 and H200 GPUs offer strong performance, and the newer B200 is anticipated to dominate in many inference workloads.</span><span class="c15">28</span><span class="c8">&nbsp;NVIDIA&#39;s TensorRT-LLM inference framework, despite past criticisms regarding developer experience, is recognized for its high performance.</span><span class="c12">28</span></p><p class="c3"><span class="c8">AMD, in contrast, generally offers more competitive pricing for its GPUs </span><span class="c15">15</span><span class="c8">&nbsp;and often holds a memory capacity advantage with its Instinct series, which is crucial for large model inference and training.</span><span class="c15">7</span><span class="c8">&nbsp;While AMD&#39;s MI300/MI325 series can outperform NVIDIA&#39;s H100 in certain workloads, particularly those sensitive to memory capacity or at ultra-low latencies, NVIDIA&#39;s overall performance per dollar often remains superior, especially in the GPU rental market.</span><span class="c12">28</span></p><p class="c3"><span class="c8">However, NVIDIA&#39;s software ecosystem, comprehensive continuous integration (CI) coverage, and advanced features like disaggregated prefill inference optimization consistently outpace AMD&#39;s offerings.</span><span class="c15">28</span><span class="c8">&nbsp;A contributing factor to AMD&#39;s weaker developer experience compared to NVIDIA is the reported lack of abundant internal R&amp;D cluster resources.</span><span class="c12">28</span></p><p class="c3"><span class="c8">The competitive landscape clearly indicates that NVIDIA&#39;s advantage extends beyond raw hardware specifications or even the CUDA API itself. It resides in the breadth and depth of its entire software ecosystem, encompassing mature tools, extensive libraries, robust CI/CD pipelines, and widespread availability in cloud environments.</span><span class="c15">15</span><span class="c6">&nbsp;This comprehensive offering creates a powerful &quot;software moat&quot; that goes beyond simple code porting. AMD&#39;s challenge, therefore, is not merely to achieve performance parity or provide a translation layer like HIP. It must build an equally comprehensive, reliable, and user-friendly software stack that minimizes the total cost of ownership for researchers, including their valuable time and effort. This requires sustained and significant investment in software engineering, developer relations, and strategic partnerships to expand its ecosystem and close the existing gaps in areas such as CI coverage and advanced inference features.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Conclusion and Recommendations</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Summary of Key Findings</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c3"><span class="c6">The analysis reveals a nuanced landscape of AMD GPU adoption and sentiment within academic, scientific, and HPC environments. AMD has achieved significant high-profile successes, notably powering the world&#39;s top exascale supercomputers, Frontier and El Capitan. These deployments underscore the robust capabilities of AMD&#39;s hardware and the foundational strength of its ROCm software stack in highly controlled, large-scale settings. The ROCm platform itself is undergoing rapid evolution, delivering substantial performance improvements and expanding compatibility with major scientific computing frameworks.</span></p><p class="c3"><span class="c6">Despite these strategic wins and continuous software development, a persistent negative sentiment exists among individual researchers and smaller institutions. This is primarily driven by practical usability challenges, including complex and unstable installation processes, recurring driver issues, and limited official support for consumer-grade GPUs. The deeply entrenched &quot;CUDA moat,&quot; built on NVIDIA&#39;s mature ecosystem, continues to pose a significant barrier to code porting and broader adoption, even with AMD&#39;s HIP efforts. While institutional IT departments face inherent integration challenges with heterogeneous systems, AMD&#39;s academic programs and direct engineering support have facilitated successful large-scale deployments in some instances.</span></p><p class="c3"><span class="c6">The overall researcher sentiment is bifurcated: highly positive for high-end, well-supported deployments, but notably negative for individual users and those relying on consumer hardware. AMD&#39;s open-source strategy for ROCm is a crucial differentiator and a considerable market opportunity, provided that the practical usability issues faced by a broader user base are effectively addressed.</span></p><p class="c3 c14"><span class="c6"></span></p><h3 class="c25"><span class="c19 c17">Actionable Recommendations for AMD to Enhance Adoption and User Experience</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c25"><span class="c6">To accelerate adoption and improve the overall user experience within the academic and HPC research communities, the following actionable recommendations are critical for AMD:</span></p><ol class="c21 lst-kix_list_2-0 start" start="1"><li class="c24 li-bullet-0"><span class="c9 c17">Prioritize the &quot;Out-of-the-Box&quot; Experience Across All Hardware Tiers:</span></li></ol><ul class="c21 lst-kix_list_3-1 start"><li class="c18 li-bullet-0"><span class="c9">Recommendation:</span><span class="c6">&nbsp;Allocate substantial engineering resources to drastically simplify ROCm installation and setup, particularly for consumer-grade GPUs and widely used Linux distributions. The objective should be to achieve &quot;one-click&quot; or pip install level ease of use for core machine learning frameworks.</span></li><li class="c23 li-bullet-0"><span class="c9">Rationale:</span><span class="c6">&nbsp;The most frequently reported pain points revolve around the complexity and instability of installation and initial setup on consumer hardware, which serves as the primary entry point for many researchers. A seamless and intuitive initial experience is fundamental for cultivating a loyal user base and effectively challenging NVIDIA&#39;s dominance at the grassroots level.</span></li></ul><ol class="c21 lst-kix_list_2-0" start="2"><li class="c29 li-bullet-0"><span class="c9 c17">Expand and Standardize Official Consumer GPU Support:</span></li></ol><ul class="c21 lst-kix_list_4-1 start"><li class="c18 li-bullet-0"><span class="c9">Recommendation:</span><span class="c6">&nbsp;Officially extend ROCm support to a broader range of consumer Radeon GPUs, ensuring consistent stability and performance across different product generations (e.g., RDNA2/3 beyond just the 7900 series). It is equally important to avoid prematurely deprecating cards that still possess ample VRAM.</span></li><li class="c23 li-bullet-0"><span class="c9">Rationale:</span><span class="c6">&nbsp;Many researchers and students rely on more affordable consumer cards for development, experimentation, and smaller-scale projects. Fragmented or unofficial support forces these users into time-consuming workarounds or custom compilations, hindering adoption and reinforcing the perception of ROCm as an unreliable or difficult platform.</span></li></ul><ol class="c21 lst-kix_list_2-0" start="3"><li class="c29 li-bullet-0"><span class="c9 c17">Enhance Documentation Depth and Transparency:</span></li></ol><ul class="c21 lst-kix_list_5-1 start"><li class="c18 li-bullet-0"><span class="c9">Recommendation:</span><span class="c6">&nbsp;Significantly improve the depth, clarity, and discoverability of documentation for advanced use cases, complex troubleshooting scenarios (e.g., multi-GPU hangs, performance regressions), and optimization strategies for specific scientific workloads. Crucially, implement transparent continuous integration/continuous delivery (CI/CD) results for popular frameworks across all supported hardware, making CI runs openly accessible to the community.</span></li><li class="c23 li-bullet-0"><span class="c9">Rationale:</span><span class="c6">&nbsp;While existing documentation covers a wide array of topics, its lack of granular detail for complex scenarios compels users to spend excessive time debugging or seeking community support. Transparent CI/CD would build trust by allowing users to verify expected performance and diagnose issues more efficiently, thereby reducing the burden on individual researchers.</span></li></ul><ol class="c21 lst-kix_list_2-0" start="4"><li class="c29 li-bullet-0"><span class="c9 c17">Increase Investment in Software Ecosystem Maturity and Third-Party Collaboration:</span></li></ol><ul class="c21 lst-kix_list_6-1 start"><li class="c18 li-bullet-0"><span class="c9">Recommendation:</span><span class="c6">&nbsp;Substantially increase investment in internal R&amp;D cluster resources to improve CI coverage and overall software quality, directly addressing reported accuracy issues. Actively contribute to and deepen collaboration with third-party machine learning and HPC projects (e.g., vLLM, bitsandbytes, OpenMM, GROMACS, LAMMPS) to ensure robust, optimized, and out-of-the-box compatibility.</span></li><li class="c23 li-bullet-0"><span class="c9">Rationale:</span><span class="c6">&nbsp;NVIDIA&#39;s formidable &quot;software moat&quot; is built upon a highly mature and integrated ecosystem. AMD must close this gap by ensuring its software components are not only functional but also highly optimized, stable, and seamlessly integrated with the tools researchers already utilize. This involves addressing issues like &quot;dumber answers&quot; due to accuracy test gaps and providing a more cohesive developer experience.</span></li></ul><ol class="c21 lst-kix_list_2-0" start="5"><li class="c29 li-bullet-0"><span class="c9 c17">Strategically Leverage Cloud and Academic Programs:</span></li></ol><ul class="c21 lst-kix_list_7-1 start"><li class="c18 li-bullet-0"><span class="c9">Recommendation:</span><span class="c6">&nbsp;Expand the AMD Developer Cloud offering to include a wider range of the latest hardware (e.g., MI350 series, MI325X) and ensure timely availability of the most current ROCm versions. Simultaneously, actively encourage cloud service providers to offer more competitive AMD GPU rental options.</span></li><li class="c23 li-bullet-0"><span class="c9">Rationale:</span><span class="c6">&nbsp;Cloud access serves as a critical &quot;try before you buy&quot; gateway for many researchers. Making AMD GPUs easily and affordably accessible in the cloud can significantly drive organic adoption and familiarization with the ROCm stack, directly influencing future procurement decisions. Continued and expanded academic donation programs remain valuable, but must be paired with sufficient post-installation technical support to ensure successful long-term utilization.</span></li></ul><p class="c28"><span class="c6">These recommendations, if systematically implemented, would address the core pain points identified by the research community, bridge the gap between AMD&#39;s high-end HPC successes and broader academic adoption, and significantly strengthen ROCm&#39;s competitive position against established ecosystems.</span></p><p class="c3 c14"><span class="c6"></span></p><h2 class="c25"><span class="c13">Appendix</span></h2><p class="c3 c14"><span class="c13"></span></p><p class="c3 c14"><span class="c13"></span></p><h3 class="c25"><span class="c19 c17">Raw Data, Sources, and Detailed Methodology</span></h3><p class="c3 c14"><span class="c19 c17"></span></p><p class="c25"><span class="c6">This report synthesizes information from a diverse collection of publicly available sources, identified and analyzed to provide a comprehensive overview of researcher experiences and sentiment toward AMD GPU software and hardware. The primary sources include:</span></p><ul class="c21 lst-kix_list_8-0 start"><li class="c25 c39 li-bullet-0"><span class="c9">Academic Forums and Mailing Lists:</span><span class="c6">&nbsp;Discussions from platforms such as Reddit (e.g., r/ROCm, r/Amd, r/learnmachinelearning) and specialized computing forums (e.g., JuliaLang Discourse, Materials Science Community Discourse). These sources provided direct user feedback, troubleshooting experiences, and community-driven workarounds.</span></li><li class="c27 li-bullet-0"><span class="c9">GitHub Issues and Discussions:</span><span class="c6">&nbsp;Repositories for scientific software projects (e.g., GROMACS, OpenMM) and ROCm components (e.g., ROCm/ROCm, ROCm/rocprofiler-compute, ROCm/Gromacs, openmm/openmm). These sources offered detailed technical issues, bug reports, feature requests, and direct interactions between users and project maintainers.</span></li><li class="c27 li-bullet-0"><span class="c9">Supercomputing Center User Documentation and User Group Meetings:</span><span class="c6">&nbsp;Documentation from national laboratories and supercomputing centers (e.g., Oak Ridge National Laboratory, Argonne National Laboratory, Purdue University&#39;s RCAC) provided insights into large-scale deployments, supported software, and integration challenges. Conference proceedings from events like SC (Supercomputing Conference) and ISC (International Supercomputing Conference) offered high-level overviews and technical presentations on AMD&#39;s role in exascale systems.</span></li><li class="c27 li-bullet-0"><span class="c9">ResearchGate, arXiv, and Institutional Reports:</span><span class="c6">&nbsp;Scholarly articles and preprints (e.g., from arXiv and ResearchGate) provided peer-reviewed analyses of performance, compatibility, and specific technical hurdles or solutions related to AMD GPUs and ROCm.</span></li><li class="c27 li-bullet-0"><span class="c9">Official AMD Communications:</span><span class="c6">&nbsp;AMD&#39;s corporate blogs, product documentation, developer hubs, and press releases offered official statements on ROCm development, hardware capabilities, strategic partnerships, and academic programs.</span></li></ul><p class="c36"><span class="c9 c17">Detailed Methodology:</span></p><ol class="c21 lst-kix_list_9-0 start" start="1"><li class="c24 li-bullet-0"><span class="c9">Data Collection and Curation:</span><span class="c6">&nbsp;The provided research material, consisting of numerous snippets with IDs and URLs, was systematically reviewed. Each snippet was categorized based on its relevance to the key areas of investigation outlined in the user query (e.g., ROCm adoption, compatibility, performance, stability, documentation, support, procurement, porting barriers).</span></li><li class="c29 li-bullet-0"><span class="c9">Sentiment Analysis:</span><span class="c6">&nbsp;Opinions and discussions within the snippets were qualitatively categorized as Positive, Neutral, or Negative.</span></li></ol><ul class="c21 lst-kix_list_a-1 start"><li class="c18 li-bullet-0"><span class="c9">Positive:</span><span class="c6">&nbsp;Indicated by reports of successful deployments, high performance, energy efficiency, growing adoption, and positive experiences with AMD programs or support.</span></li><li class="c23 li-bullet-0"><span class="c9">Negative:</span><span class="c6">&nbsp;Characterized by reports of installation difficulties, driver issues, instability, compatibility problems, performance regressions, steep learning curves, and frustrations with support or documentation gaps.</span></li><li class="c23 li-bullet-0"><span class="c9">Neutral:</span><span class="c6">&nbsp;Represented by factual statements, technical specifications, or discussions that acknowledge both strengths and weaknesses without expressing a strong positive or negative bias.</span></li><li class="c23 li-bullet-0"><span class="c6">The categorization was supported by explicit quotes or summaries of reported experiences.</span></li></ul><ol class="c21 lst-kix_list_9-0" start="3"><li class="c29 li-bullet-0"><span class="c9">Pain Point Identification and Ranking:</span><span class="c6">&nbsp;Specific issues mentioned by researchers and IT professionals were extracted and grouped into common themes (e.g., &quot;Installation &amp; Setup Difficulties,&quot; &quot;Limited Consumer GPU Support&quot;). The frequency and prominence of these mentions across various sources (e.g., number of Reddit threads, GitHub issues, or explicit mentions in academic papers) were used to rank them.</span></li><li class="c29 li-bullet-0"><span class="c9">Solution Suggestions:</span><span class="c6">&nbsp;Researcher-proposed fixes, workarounds, and recommendations for improvement were extracted and compiled, directly addressing the identified pain points.</span></li><li class="c29 li-bullet-0"><span class="c9">Success Stories Documentation:</span><span class="c6">&nbsp;Instances of successful AMD GPU deployments, research breakthroughs enabled by AMD hardware/software, and positive academic collaborations were documented.</span></li><li class="c29 li-bullet-0"><span class="c9">Trend Analysis:</span><span class="c6">&nbsp;Historical data (2018-2021) was compared with recent data (2022-2025) to identify shifts in adoption rates, technology evolution, and changes in sentiment. This included tracking AMD&#39;s position in the Top500 list and the evolution of ROCm versions and features.</span></li><li class="c29 li-bullet-0"><span class="c9">Demographic Insights:</span><span class="c6">&nbsp;Where available, information on the type of institution (university lab, research institute, national supercomputing center), field of research, and regional differences was noted to provide context to the experiences.</span></li><li class="c29 li-bullet-0"><span class="c9">Quality Assurance:</span><span class="c6">&nbsp;Throughout the analysis, emphasis was placed on prioritizing recent, detailed technical feedback. Both positive and negative perspectives were included to ensure balance. Focus was maintained on actionable, specific issues rather than vague criticisms. Claims were corroborated across multiple sources or verified against official technical documentation where possible. A clear distinction was maintained between hardware-related and software-related issues.</span></li></ol><p class="c36"><span class="c9 c17">Limitations:</span></p><ul class="c21 lst-kix_list_b-0 start"><li class="c25 c39 li-bullet-0"><span class="c9">Data Scope:</span><span class="c6">&nbsp;The report is limited by the provided research snippets. While diverse, they may not represent the entirety of researcher experiences globally.</span></li><li class="c27 li-bullet-0"><span class="c9">Sentiment Subjectivity:</span><span class="c6">&nbsp;While efforts were made to objectively categorize sentiment, qualitative data inherently carries some degree of subjectivity.</span></li><li class="c27 li-bullet-0"><span class="c9">Proprietary Information:</span><span class="c6">&nbsp;The report relies on publicly available information. Internal AMD data or detailed performance metrics from private research labs are not included unless they were published in the provided snippets.</span></li><li class="c27 li-bullet-0"><span class="c9">Real-time Dynamics:</span><span class="c6">&nbsp;The technology landscape in HPC and AI evolves rapidly. While the report covers up to mid-2025 based on forward-looking statements in some snippets, the actual implementation and user experiences may continue to change.</span></li></ul><p class="c38"><span class="c6">This methodology ensures a rigorous, evidence-based approach to understanding the complex interplay of hardware, software, and user experience in the adoption of AMD GPUs within the scientific and HPC communities.</span></p><h4 class="c31"><span class="c22 c34">Works cited</span></h4><ol class="c21 lst-kix_list_c-0 start" start="1"><li class="c1 li-bullet-0"><span class="c11">AMD Powered El Capitan and Frontier Maintain Global Supercomputing Leadership, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/blogs/2025/amd-maintains-global-supercomputing-leadership.html&amp;sa=D&amp;source=editors&amp;ust=1750774354901652&amp;usg=AOvVaw01OLqQPo-0VPEahL19Wpq2">https://www.amd.com/en/blogs/2025/amd-maintains-global-supercomputing-leadership.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Supercomputing &amp; Research Solutions - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/solutions/supercomputing-and-research.html&amp;sa=D&amp;source=editors&amp;ust=1750774354902283&amp;usg=AOvVaw0vk4w-TTkFLUKStrOSlpjG">https://www.amd.com/en/solutions/supercomputing-and-research.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm: Powering the World&#39;s Fastest Supercomputers, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-power/README.html&amp;sa=D&amp;source=editors&amp;ust=1750774354902934&amp;usg=AOvVaw1gpBq-StWEGUXzEeYyeQpW">https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-power/README.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">TOP500 - Wikipedia, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/TOP500&amp;sa=D&amp;source=editors&amp;ust=1750774354903291&amp;usg=AOvVaw0mIM60Ss0kyQLM5r4lBMes">https://en.wikipedia.org/wiki/TOP500</a></span></li><li class="c1 li-bullet-0"><span class="c11">Analysis of AMD&#39;s Technical Advantages in the Global Top500 Supercomputers Ranking, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.latitudeds.com/post/analysis-of-amd-s-technical-advantages-in-the-global-top500-supercomputers-ranking&amp;sa=D&amp;source=editors&amp;ust=1750774354904043&amp;usg=AOvVaw20gU77KfF0cAEc5yiz4TED">https://www.latitudeds.com/post/analysis-of-amd-s-technical-advantages-in-the-global-top500-supercomputers-ranking</a></span></li><li class="c1 li-bullet-0"><span class="c11">Enabling the Future of AI: Introducing AMD ROCm 7 and AMD Developer Cloud, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/blogs/2025/enabling-the-future-of-ai-introducing-amd-rocm-7-and-the-amd-developer-cloud.html&amp;sa=D&amp;source=editors&amp;ust=1750774354904679&amp;usg=AOvVaw2D6G7knB4p_CcyVc4-CkZt">https://www.amd.com/en/blogs/2025/enabling-the-future-of-ai-introducing-amd-rocm-7-and-the-amd-developer-cloud.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Instinct MI350 Series and Beyond: Accelerating the Future of AI and HPC, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/blogs/2025/amd-instinct-mi350-series-and-beyond-accelerating-the-future-of-ai-and-hpc.html&amp;sa=D&amp;source=editors&amp;ust=1750774354905303&amp;usg=AOvVaw2XUQIVKI7wdUl7exs1-iWv">https://www.amd.com/en/blogs/2025/amd-instinct-mi350-series-and-beyond-accelerating-the-future-of-ai-and-hpc.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Posted in 2025 - AMD ROCm&trade; Blogs, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/blog/2025.html&amp;sa=D&amp;source=editors&amp;ust=1750774354905791&amp;usg=AOvVaw2VBjYlLrD7A4KeZlHEBCYF">https://rocm.blogs.amd.com/blog/2025.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm 6.4: Breaking Barriers in AI, HPC, and Modular GPU Software, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-6.4-blog/README.html&amp;sa=D&amp;source=editors&amp;ust=1750774354906283&amp;usg=AOvVaw2Jkkl8ZqagGBN2SWm86iEA">https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-6.4-blog/README.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">What&#39;s New in AMD ROCm 6.4: Breakthrough Inference, Plug-and-Play Containers &amp; Modular Deployment for Scalable AI on AMD Instinct GPUs, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/blogs/2025/what-s-new-in-amd-rocm-6-4-breakthrough-inference.html&amp;sa=D&amp;source=editors&amp;ust=1750774354906959&amp;usg=AOvVaw3iK4h1EXN30qP8Ff-C6tFo">https://www.amd.com/en/blogs/2025/what-s-new-in-amd-rocm-6-4-breakthrough-inference.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm&trade; Software, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/es/products/software/rocm.html%23!&amp;sa=D&amp;source=editors&amp;ust=1750774354907369&amp;usg=AOvVaw0d4dAsTa60zabsJCPFOyu4">https://www.amd.com/es/products/software/rocm.html#!</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD&#39;s AI Surge Challenges Nvidia&#39;s Dominance | Analysis - TechNewsWorld, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.technewsworld.com/story/amds-ai-surge-challenges-nvidias-dominance-179781.html&amp;sa=D&amp;source=editors&amp;ust=1750774354907982&amp;usg=AOvVaw26P0GGV4gYB6LLe5-GmuKw">https://www.technewsworld.com/story/amds-ai-surge-challenges-nvidias-dominance-179781.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm Revisited: Evolution of the High-Performance GPU Computing Ecosystem, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-ecosy/README.html&amp;sa=D&amp;source=editors&amp;ust=1750774354908481&amp;usg=AOvVaw1GxA2BPoZ2MViTagfUyh5X">https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-ecosy/README.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD at Computex 2025: Making the Case for an AI Powerhouse - Hardware Busters, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://hwbusters.com/news/amd-at-computex-2025-making-the-case-for-an-ai-powerhouse/&amp;sa=D&amp;source=editors&amp;ust=1750774354908983&amp;usg=AOvVaw08ttIuA5bZwjKh3kUMwsV4">https://hwbusters.com/news/amd-at-computex-2025-making-the-case-for-an-ai-powerhouse/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Vs NVIDIA: Which GPU Fits Your Business In 2025? - AceCloud, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://acecloud.ai/blog/amd-vs-nvidia/&amp;sa=D&amp;source=editors&amp;ust=1750774354909376&amp;usg=AOvVaw2JGwGVr8Iw8lPJpMx46OE4">https://acecloud.ai/blog/amd-vs-nvidia/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Introducing Driver Experiments - AMD GPUOpen, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://gpuopen.com/learn/rdts-driver-experiments/&amp;sa=D&amp;source=editors&amp;ust=1750774354909792&amp;usg=AOvVaw1zSc60bcjKtQTYjbZ5fMMx">https://gpuopen.com/learn/rdts-driver-experiments/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Installing AMD Drivers. What&#39;s the secret? - Support and Help - Ubuntu Discourse, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://discourse.ubuntu.com/t/installing-amd-drivers-whats-the-secret/51180&amp;sa=D&amp;source=editors&amp;ust=1750774354910341&amp;usg=AOvVaw0-FCg_ACfpGWK6aqoaF1ft">https://discourse.ubuntu.com/t/installing-amd-drivers-whats-the-secret/51180</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMDGPU.jl has made such amazing progress over the last year! - GPU - Julia Discourse, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://discourse.julialang.org/t/amdgpu-jl-has-made-such-amazing-progress-over-the-last-year/75333&amp;sa=D&amp;source=editors&amp;ust=1750774354910994&amp;usg=AOvVaw0AWGwFG6IRSjYbvCSIve4u">https://discourse.julialang.org/t/amdgpu-jl-has-made-such-amazing-progress-over-the-last-year/75333</a></span></li><li class="c1 li-bullet-0"><span class="c11">How on earth do I set up rocm : r/ROCm - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1kkfrh6/how_on_earth_do_i_set_up_rocm/&amp;sa=D&amp;source=editors&amp;ust=1750774354911434&amp;usg=AOvVaw0g1kSPIDRS8luntcMYkVQh">https://www.reddit.com/r/ROCm/comments/1kkfrh6/how_on_earth_do_i_set_up_rocm/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Problem after installing rocm - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1isrv96/problem_after_installing_rocm/&amp;sa=D&amp;source=editors&amp;ust=1750774354911901&amp;usg=AOvVaw3EZgprDyyamCOiCi4BsNY-">https://www.reddit.com/r/ROCm/comments/1isrv96/problem_after_installing_rocm/</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCM Feedback for AMD - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1i5aatx/rocm_feedback_for_amd/&amp;sa=D&amp;source=editors&amp;ust=1750774354912255&amp;usg=AOvVaw37vUo2SV0ADsRcmLpsv5L3">https://www.reddit.com/r/ROCm/comments/1i5aatx/rocm_feedback_for_amd/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Installation troubleshooting &mdash; ROCm installation (Linux), accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/install-faq.html&amp;sa=D&amp;source=editors&amp;ust=1750774354912785&amp;usg=AOvVaw0aSJ32gVtmExuHosU_aae7">https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/install-faq.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Is AMD starting to bridge the CUDA moat? : r/ROCm - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1i0k8id/is_amd_starting_to_bridge_the_cuda_moat/&amp;sa=D&amp;source=editors&amp;ust=1750774354913162&amp;usg=AOvVaw2QTwvkpOCRMLymvIy4Q56e">https://www.reddit.com/r/ROCm/comments/1i0k8id/is_amd_starting_to_bridge_the_cuda_moat/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Transformer Lab Now Works with AMD GPUs, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://transformerlab.ai/blog/amd-support/&amp;sa=D&amp;source=editors&amp;ust=1750774354913499&amp;usg=AOvVaw2YsWvlJbOBfWU-aUmM6K6Y">https://transformerlab.ai/blog/amd-support/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Preliminary report:Initial evaluation of StdPar implementations on AMD GPUs for HPC, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://arxiv.org/html/2401.02680v1&amp;sa=D&amp;source=editors&amp;ust=1750774354913894&amp;usg=AOvVaw3ra5D5Oep-sQNTcl2MCiZY">https://arxiv.org/html/2401.02680v1</a></span></li><li class="c1 li-bullet-0"><span class="c11">Follow up on ROCm feedback thread - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1i92r6q/follow_up_on_rocm_feedback_thread/&amp;sa=D&amp;source=editors&amp;ust=1750774354914204&amp;usg=AOvVaw1MyHtBPm_0YuYbZyN8ppIj">https://www.reddit.com/r/ROCm/comments/1i92r6q/follow_up_on_rocm_feedback_thread/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Transitioning from CUDA to ROCm - Scimus, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://thescimus.com/blog/transitioning-from-cuda-to-rocm/&amp;sa=D&amp;source=editors&amp;ust=1750774354914596&amp;usg=AOvVaw1nZwl90Y0slPsF1lcR300x">https://thescimus.com/blog/transitioning-from-cuda-to-rocm/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD vs NVIDIA Inference Benchmark: Who Wins? &ndash; Performance &amp; Cost Per Million Tokens, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://semianalysis.com/2025/05/23/amd-vs-nvidia-inference-benchmark-who-wins-performance-cost-per-million-tokens/&amp;sa=D&amp;source=editors&amp;ust=1750774354915100&amp;usg=AOvVaw3upsE6hkOIvXAbBMYOJDJE">https://semianalysis.com/2025/05/23/amd-vs-nvidia-inference-benchmark-who-wins-performance-cost-per-million-tokens/</a></span></li><li class="c1 li-bullet-0"><span class="c11">And yet they still can&#39;t solve the problem of their GPU driver/software stack fo... | Hacker News, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://news.ycombinator.com/item?id%3D40702743&amp;sa=D&amp;source=editors&amp;ust=1750774354915493&amp;usg=AOvVaw26j4SDbJ-SU8rypTkTPMs-">https://news.ycombinator.com/item?id=40702743</a></span></li><li class="c1 li-bullet-0"><span class="c11">Hardware Noob: is AMD ROCm as usable as NVIDA Cuda : r/learnmachinelearning - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/learnmachinelearning/comments/1jf95af/hardware_noob_is_amd_rocm_as_usable_as_nvida_cuda/&amp;sa=D&amp;source=editors&amp;ust=1750774354916240&amp;usg=AOvVaw3RDg9JQPvCIjahoYMzqnOx">https://www.reddit.com/r/learnmachinelearning/comments/1jf95af/hardware_noob_is_amd_rocm_as_usable_as_nvida_cuda/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm 7.0 To Align HIP C++ &quot;Even More Closely With CUDA&quot; - Phoronix, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.phoronix.com/news/AMD-ROCm-7.0-HIP-Plans&amp;sa=D&amp;source=editors&amp;ust=1750774354916687&amp;usg=AOvVaw0J1lW6fqa8b_aE06w07ECQ">https://www.phoronix.com/news/AMD-ROCm-7.0-HIP-Plans</a></span></li><li class="c1 li-bullet-0"><span class="c11">Overcome Common HPC Challenges Including Platform Complexity - Penguin Solutions, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.penguinsolutions.com/en-us/resources/blog/hpc-challenges-overcoming-platform-complexity&amp;sa=D&amp;source=editors&amp;ust=1750774354917223&amp;usg=AOvVaw1qJEdum9Tb_KIunOOFq0uz">https://www.penguinsolutions.com/en-us/resources/blog/hpc-challenges-overcoming-platform-complexity</a></span></li><li class="c1 li-bullet-0"><span class="c11">AI will Transform the Enterprise. But There are Some Tough Infrastructure Challenges to Solve First - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/solutions/data-center/insights/ai-will-transform-the-enterprise-but-there-are-some-tough-infrastructure-challenges-to-solve-first.html&amp;sa=D&amp;source=editors&amp;ust=1750774354918123&amp;usg=AOvVaw0mowEjmhudyuJM4uHaJfQP">https://www.amd.com/en/solutions/data-center/insights/ai-will-transform-the-enterprise-but-there-are-some-tough-infrastructure-challenges-to-solve-first.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AI &amp; HPC Cloud Access - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/developer/resources/cloud-access.html&amp;sa=D&amp;source=editors&amp;ust=1750774354918610&amp;usg=AOvVaw1XPQz_jqjXj3W19SFdBpDz">https://www.amd.com/en/developer/resources/cloud-access.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD University Program - Donation Program, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/corporate/university-program/donation-program.html&amp;sa=D&amp;source=editors&amp;ust=1750774354919159&amp;usg=AOvVaw0Zq579k_0X2GvdNDKTVR8j">https://www.amd.com/en/corporate/university-program/donation-program.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Chile&#39;s NLHPC supercharged research performance with AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/resources/case-studies/chile-nlhpc.html&amp;sa=D&amp;source=editors&amp;ust=1750774354919674&amp;usg=AOvVaw1g08PkrnWIYRgh8ajTUgpV">https://www.amd.com/en/resources/case-studies/chile-nlhpc.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">University of Tartu Achieves HPC Excellence with AMD EPYC&trade;, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/resources/case-studies/university-of-tartu.html&amp;sa=D&amp;source=editors&amp;ust=1750774354920257&amp;usg=AOvVaw2YyWG643oCW9iHLH9uTgRE">https://www.amd.com/en/resources/case-studies/university-of-tartu.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm&trade; Software - GitHub Home, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/ROCm/ROCm&amp;sa=D&amp;source=editors&amp;ust=1750774354920695&amp;usg=AOvVaw3V9ew1xJdtZnuODa6L0Dm7">https://github.com/ROCm/ROCm</a></span></li><li class="c1 li-bullet-0"><span class="c11">Powering the AI Engine: AMD Instinct, ROCm and Real-World Wins - Six Five On The Road, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://futurumgroup.com/insights/powering-the-ai-engine-amd-instinct-rocm-and-real-world-wins-six-five-on-the-road/&amp;sa=D&amp;source=editors&amp;ust=1750774354921386&amp;usg=AOvVaw1gHwWL2yEJrUuARMsP4z_B">https://futurumgroup.com/insights/powering-the-ai-engine-amd-instinct-rocm-and-real-world-wins-six-five-on-the-road/</a></span></li><li class="c1 li-bullet-0"><span class="c11">The role of ROCm in AMD&#39;s future : r/AMD_Stock - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/AMD_Stock/comments/1i1etc0/the_role_of_rocm_in_amds_future/&amp;sa=D&amp;source=editors&amp;ust=1750774354921999&amp;usg=AOvVaw0EVBaRYbkLAvlQQRdjNdxQ">https://www.reddit.com/r/AMD_Stock/comments/1i1etc0/the_role_of_rocm_in_amds_future/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD workshop draws leading experts in high-performance computing, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://scds.uoregon.edu/amd-workshop-draws-leading-experts-high-performance-computing&amp;sa=D&amp;source=editors&amp;ust=1750774354922702&amp;usg=AOvVaw2OeAQ8k9HSNpM_6U7LDF7K">https://scds.uoregon.edu/amd-workshop-draws-leading-experts-high-performance-computing</a></span></li><li class="c1 li-bullet-0"><span class="c11">At ISC 2025: Goethe University Discusses AI-Powered Scientific Research on the VDURA Data Platform | Inside HPC &amp; AI News, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://insidehpc.com/2025/06/at-isc-2025-goethe-university-discusses-ai-powered-scientific-research-on-the-vdura-data-platform/&amp;sa=D&amp;source=editors&amp;ust=1750774354923669&amp;usg=AOvVaw3cPBCRsIoNKWhAlT5LRBmz">https://insidehpc.com/2025/06/at-isc-2025-goethe-university-discusses-ai-powered-scientific-research-on-the-vdura-data-platform/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Trying Out The AMD Developer Cloud For Quickly Evaluating Instinct + ROCm - Phoronix, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.phoronix.com/review/amd-developer-cloud&amp;sa=D&amp;source=editors&amp;ust=1750774354924354&amp;usg=AOvVaw21xbze-RPafwFtgXNvhzMC">https://www.phoronix.com/review/amd-developer-cloud</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Showcases HPC Growth at SC21 - HPCwire, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hpcwire.com/off-the-wire/amd-showcases-hpc-growth-at-sc21/&amp;sa=D&amp;source=editors&amp;ust=1750774354925028&amp;usg=AOvVaw0XZTcG32-Fa6zcOT5dfjWQ">https://www.hpcwire.com/off-the-wire/amd-showcases-hpc-growth-at-sc21/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Learning to Scale the Summit: AI for Science on a Leadership Supercomputer - OSTI, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.osti.gov/servlets/purl/2076211&amp;sa=D&amp;source=editors&amp;ust=1750774354925627&amp;usg=AOvVaw0h1dm47FOabwMF4kPrx7ss">https://www.osti.gov/servlets/purl/2076211</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm&trade; Software, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/products/software/rocm.html&amp;sa=D&amp;source=editors&amp;ust=1750774354926115&amp;usg=AOvVaw1OxHpyMX-75__d_Ce9-r_l">https://www.amd.com/en/products/software/rocm.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Designing a ROCm-Aware MPI Library for AMD GPUs: Early Experiences - ResearchGate, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.researchgate.net/publication/352472901_Designing_a_ROCm-Aware_MPI_Library_for_AMD_GPUs_Early_Experiences&amp;sa=D&amp;source=editors&amp;ust=1750774354926934&amp;usg=AOvVaw35_WLVmwrQ4ceIVTC33-V9">https://www.researchgate.net/publication/352472901_Designing_a_ROCm-Aware_MPI_Library_for_AMD_GPUs_Early_Experiences</a></span></li><li class="c1 li-bullet-0"><span class="c11">High-Performance and Scalable Middleware for HPC, AI and Data Science on Heterogenous Systems, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.pccluster.org/ja/event/data/230419_pccc_wsAI_2_panda.pdf&amp;sa=D&amp;source=editors&amp;ust=1750774354927539&amp;usg=AOvVaw1yW222pW_-8_ECz8B6H12c">https://www.pccluster.org/ja/event/data/230419_pccc_wsAI_2_panda.pdf</a></span></li><li class="c1 li-bullet-0"><span class="c11">Designing High-Performance and Scalable Middleware for HPC, AI and Data Science based on MPI - MVAPICH, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://mvapich.cse.ohio-state.edu/static/media/talks/slide/exampi_keynote_talk_dk.pdf&amp;sa=D&amp;source=editors&amp;ust=1750774354928295&amp;usg=AOvVaw2KrIEBbCTeZ-NK46rauLDk">https://mvapich.cse.ohio-state.edu/static/media/talks/slide/exampi_keynote_talk_dk.pdf</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm - Phoronix, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.phoronix.com/search/ROCm&amp;sa=D&amp;source=editors&amp;ust=1750774354928722&amp;usg=AOvVaw3uk7y8d1-t8_GKvWyLt38q">https://www.phoronix.com/search/ROCm</a></span></li><li class="c1 li-bullet-0"><span class="c11">TensorFlow compatibility - ROCm Documentation - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/en/latest/compatibility/ml-compatibility/tensorflow-compatibility.html&amp;sa=D&amp;source=editors&amp;ust=1750774354929369&amp;usg=AOvVaw2KB0WsT5uPHMkSTTjWzYaj">https://rocm.docs.amd.com/en/latest/compatibility/ml-compatibility/tensorflow-compatibility.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Part I: Is AMD ROCm&trade; Ready to Deploy Leading AI Workloads?, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://infohub.delltechnologies.com/p/part-i-is-amd-rocm-tm-ready-to-deploy-leading-ai-workloads/&amp;sa=D&amp;source=editors&amp;ust=1750774354930041&amp;usg=AOvVaw1Pt3vhVwoMFKPinu5ivojB">https://infohub.delltechnologies.com/p/part-i-is-amd-rocm-tm-ready-to-deploy-leading-ai-workloads/</a></span></li><li class="c1 li-bullet-0"><span class="c11">TensorFlow on ROCm &mdash; ROCm installation (Linux), accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html&amp;sa=D&amp;source=editors&amp;ust=1750774354930674&amp;usg=AOvVaw2ZdMEftYYsO7wOruP8FTUa">https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Using ROCm for HPC, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/en/docs-6.4.0/how-to/rocm-for-hpc/index.html&amp;sa=D&amp;source=editors&amp;ust=1750774354931143&amp;usg=AOvVaw2-1KRNS41uv1pWx2KqEFMm">https://rocm.docs.amd.com/en/docs-6.4.0/how-to/rocm-for-hpc/index.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Knowledge Base: AMD ROCm containers: lammps - RCAC, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.rcac.purdue.edu/knowledge/rocm/lammps&amp;sa=D&amp;source=editors&amp;ust=1750774354931670&amp;usg=AOvVaw1baa24hYjjAwv_iLfTWyB8">https://www.rcac.purdue.edu/knowledge/rocm/lammps</a></span></li><li class="c1 li-bullet-0"><span class="c11">Studying Performance Portability of LAMMPS across Diverse GPU-based Platforms - OSTI, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.osti.gov/servlets/purl/1869068&amp;sa=D&amp;source=editors&amp;ust=1750774354932219&amp;usg=AOvVaw1LJAI0hTR4mAvl4TEwmS1A">https://www.osti.gov/servlets/purl/1869068</a></span></li><li class="c1 li-bullet-0"><span class="c11">LAMMPS - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/developer/zen-software-studio/applications/spack/hpc-applications-lammps.html&amp;sa=D&amp;source=editors&amp;ust=1750774354932794&amp;usg=AOvVaw1LGwnPCj6_K6wkRBP7Dg6M">https://www.amd.com/en/developer/zen-software-studio/applications/spack/hpc-applications-lammps.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Performance improvements - GROMACS 2025.0 documentation, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://manual.gromacs.org/documentation/2025.0/release-notes/2023/major/performance.html&amp;sa=D&amp;source=editors&amp;ust=1750774354933411&amp;usg=AOvVaw1jDN4bmowqu7CeoJH4SHzx">https://manual.gromacs.org/documentation/2025.0/release-notes/2023/major/performance.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Installation guide - GROMACS 2025.2 documentation, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://manual.gromacs.org/current/install-guide/index.html&amp;sa=D&amp;source=editors&amp;ust=1750774354933953&amp;usg=AOvVaw3s9mo2JKJxJQmtoDPaYFXk">https://manual.gromacs.org/current/install-guide/index.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm&#39;s implementation of Gromacs - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/ROCm/Gromacs&amp;sa=D&amp;source=editors&amp;ust=1750774354934469&amp;usg=AOvVaw2nC6zI6ZoqEksRpm4OrjU5">https://github.com/ROCm/Gromacs</a></span></li><li class="c1 li-bullet-0"><span class="c11">GROMACS on AMD GPU-Based HPC Platforms: Using SYCL for Performance and Portability - arXiv, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://arxiv.org/html/2405.01420v1&amp;sa=D&amp;source=editors&amp;ust=1750774354934951&amp;usg=AOvVaw07Qfz5OxRynI6LItTkkRHf">https://arxiv.org/html/2405.01420v1</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm HPC Developer Hub - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/developer/resources/rocm-hub/dev-hpc.html&amp;sa=D&amp;source=editors&amp;ust=1750774354935442&amp;usg=AOvVaw0y2jtGQYr2KM8pfQZYKXwt">https://www.amd.com/en/developer/resources/rocm-hub/dev-hpc.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">GROMACS - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/developer/zen-software-studio/applications/spack/hpc-applications-gromacs.html&amp;sa=D&amp;source=editors&amp;ust=1750774354935984&amp;usg=AOvVaw1K_gwSB7NM84dH-l2F55Df">https://www.amd.com/en/developer/zen-software-studio/applications/spack/hpc-applications-gromacs.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">OpenMM - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/developer/resources/infinity-hub/openmm.html&amp;sa=D&amp;source=editors&amp;ust=1750774354936418&amp;usg=AOvVaw0q8Y7cd4VISr1yoE29FYu3">https://www.amd.com/en/developer/resources/infinity-hub/openmm.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">2. The OpenMM Application Layer: Introduction, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=http://docs.openmm.org/7.0.0/userguide/application.html&amp;sa=D&amp;source=editors&amp;ust=1750774354937098&amp;usg=AOvVaw1eHNbVWfjKiySSi3J4aSCT">http://docs.openmm.org/7.0.0/userguide/application.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">HPC Benchmarking Results - HECBioSim, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hecbiosim.ac.uk/access-hpc/hpc-benchmarking&amp;sa=D&amp;source=editors&amp;ust=1750774354937556&amp;usg=AOvVaw0XUh07KGM6QTfQaWRKlmwB">https://www.hecbiosim.ac.uk/access-hpc/hpc-benchmarking</a></span></li><li class="c1 li-bullet-0"><span class="c11">RCAC - Software - Rosen Center for Advanced Computing - Purdue University, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rcac.purdue.edu/software?type%3Drocm&amp;sa=D&amp;source=editors&amp;ust=1750774354938118&amp;usg=AOvVaw3LBqluK7-1yWs1jLzm5nk8">https://rcac.purdue.edu/software?type=rocm</a></span></li><li class="c1 li-bullet-0"><span class="c11">Knowledge Base: AMD ROCm containers: openmm - RCAC, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.rcac.purdue.edu/knowledge/rocm/openmm&amp;sa=D&amp;source=editors&amp;ust=1750774354938606&amp;usg=AOvVaw2eb6euCH5qR_kvIuF155eA">https://www.rcac.purdue.edu/knowledge/rocm/openmm</a></span></li><li class="c1 li-bullet-0"><span class="c11">8.2 release &middot; Issue #4659 &middot; openmm/openmm - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/openmm/openmm/issues/4659&amp;sa=D&amp;source=editors&amp;ust=1750774354939071&amp;usg=AOvVaw1SPRe59-kJm0ax6wu9GUYK">https://github.com/openmm/openmm/issues/4659</a></span></li><li class="c1 li-bullet-0"><span class="c11">Molecular Simulation: OpenMM Benchmark on 25 Consumer GPUs, 95% Less Cost, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://blog.salad.com/openmm-gpu-benchmark/&amp;sa=D&amp;source=editors&amp;ust=1750774354939745&amp;usg=AOvVaw36D8BPZfgt38Nl3pMWXVVe">https://blog.salad.com/openmm-gpu-benchmark/</a></span></li><li class="c1 li-bullet-0"><span class="c11">ISOLDE Performance Benchmarks, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://tristanic.github.io/isolde/performance-benchmarks/index.html&amp;sa=D&amp;source=editors&amp;ust=1750774354940214&amp;usg=AOvVaw3q_moIgjSXhVD6l3pfDpev">https://tristanic.github.io/isolde/performance-benchmarks/index.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Migration Guide: NVIDIA to AMD &mdash; AMD Container Runtime Toolkit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://instinct.docs.amd.com/projects/container-toolkit/en/latest/container-runtime/migration-guide.html&amp;sa=D&amp;source=editors&amp;ust=1750774354940822&amp;usg=AOvVaw2WxTghNzuHvzZhjdTfCXlo">https://instinct.docs.amd.com/projects/container-toolkit/en/latest/container-runtime/migration-guide.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm Revisited: Getting Started with HIP, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-hip/README.html&amp;sa=D&amp;source=editors&amp;ust=1750774354941434&amp;usg=AOvVaw1BDknbVlT7t_7YS6_HnUN5">https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-revisited-hip/README.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD ROCm does not support the AMD Ryzen AI 300 Series GPUs - Framework Community, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://community.frame.work/t/amd-rocm-does-not-support-the-amd-ryzen-ai-300-series-gpus/68767&amp;sa=D&amp;source=editors&amp;ust=1750774354942202&amp;usg=AOvVaw1feghdN41WSNvTpTdF328p">https://community.frame.work/t/amd-rocm-does-not-support-the-amd-ryzen-ai-300-series-gpus/68767</a></span></li><li class="c1 li-bullet-0"><span class="c11">Issues &middot; ROCm/Gromacs - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/ROCm/Gromacs/issues&amp;sa=D&amp;source=editors&amp;ust=1750774354942615&amp;usg=AOvVaw040vluUd6LsW-8g7MwzJnc">https://github.com/ROCm/Gromacs/issues</a></span></li><li class="c1 li-bullet-0"><span class="c11">Issues &middot; ROCm/ROCm - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/ROCm/ROCm/issues&amp;sa=D&amp;source=editors&amp;ust=1750774354943212&amp;usg=AOvVaw27L4UMY_i3FUpSLikWO3mM">https://github.com/ROCm/ROCm/issues</a></span></li><li class="c1 li-bullet-0"><span class="c11">lammps+rocm misses rpath &middot; Issue #49984 - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/spack/spack/issues/49984&amp;sa=D&amp;source=editors&amp;ust=1750774354943758&amp;usg=AOvVaw1_hQpEbmC74QHQ9CVL_96C">https://github.com/spack/spack/issues/49984</a></span></li><li class="c1 li-bullet-0"><span class="c11">Building LAMMPS on ROCm Docker image for pytorch, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://matsci.org/t/building-lammps-on-rocm-docker-image-for-pytorch/53008&amp;sa=D&amp;source=editors&amp;ust=1750774354944375&amp;usg=AOvVaw1hXPiTcm5TINke36YKuFzL">https://matsci.org/t/building-lammps-on-rocm-docker-image-for-pytorch/53008</a></span></li><li class="c1 li-bullet-0"><span class="c11">Issues &middot; openmm/openmm - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/openmm/openmm/issues&amp;sa=D&amp;source=editors&amp;ust=1750774354944925&amp;usg=AOvVaw30bWIE-KkEaoigVf5vCSAE">https://github.com/openmm/openmm/issues</a></span></li><li class="c1 li-bullet-0"><span class="c11">[ROCm] PyTorch slow on TTS &middot; Issue #150168 - GitHub, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://github.com/pytorch/pytorch/issues/150168&amp;sa=D&amp;source=editors&amp;ust=1750774354945469&amp;usg=AOvVaw3VbxtwiugtSgmih7s7-yMS">https://github.com/pytorch/pytorch/issues/150168</a></span></li><li class="c1 li-bullet-0"><span class="c11">SCALE Benchmark case study: GROMACS : r/ROCm - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1kr5ds8/scale_benchmark_case_study_gromacs/&amp;sa=D&amp;source=editors&amp;ust=1750774354946106&amp;usg=AOvVaw1PP8nNCS-VYAg9e9Q6S87p">https://www.reddit.com/r/ROCm/comments/1kr5ds8/scale_benchmark_case_study_gromacs/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities - arXiv, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://arxiv.org/html/2506.13114v1&amp;sa=D&amp;source=editors&amp;ust=1750774354946717&amp;usg=AOvVaw23Xub6YO9u2tThn33lshkY">https://arxiv.org/html/2506.13114v1</a></span></li><li class="c1 li-bullet-0"><span class="c11">Supermicro Adds AMD Instinct MI350 GPUs to Liquid- and Air-Cooled AI Systems - HPCwire, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hpcwire.com/off-the-wire/supermicro-adds-amd-instinct-mi350-gpus-to-liquid-and-air-cooled-ai-systems/&amp;sa=D&amp;source=editors&amp;ust=1750774354947471&amp;usg=AOvVaw2oBEWHorxeJCkaGF6Wf4C9">https://www.hpcwire.com/off-the-wire/supermicro-adds-amd-instinct-mi350-gpus-to-liquid-and-air-cooled-ai-systems/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Instinct&trade; Accelerators, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/products/accelerators/instinct.html&amp;sa=D&amp;source=editors&amp;ust=1750774354947893&amp;usg=AOvVaw1et5_9ALbgcC58MQLL9ZUL">https://www.amd.com/en/products/accelerators/instinct.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AURORA EXPERIENCES AND ALCF UPDATE - HPC User Forum, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hpcuserforum.com/wp-content/uploads/2025/04/David-Martin_Argonne-National-Lab_Aurora-Experiences-and-ALCF-Update-1.pdf&amp;sa=D&amp;source=editors&amp;ust=1750774354948723&amp;usg=AOvVaw1vzko3g7vgT44nn0S5q2jP">https://www.hpcuserforum.com/wp-content/uploads/2025/04/David-Martin_Argonne-National-Lab_Aurora-Experiences-and-ALCF-Update-1.pdf</a></span></li><li class="c1 li-bullet-0"><span class="c11">Unleash Full GPU Potential: Overlap Communication and Computation with Triton-Distributed - AMD ROCm&trade; Blogs, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/software-tools-optimization/triton-distributed-c/README.html&amp;sa=D&amp;source=editors&amp;ust=1750774354949521&amp;usg=AOvVaw3fkEkmsC4emyL0JV8P5UyF">https://rocm.blogs.amd.com/software-tools-optimization/triton-distributed-c/README.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">The Best GPUs for Scientific Research | Atlantic.Net, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.atlantic.net/gpu-server-hosting/the-best-gpus-for-scientific-research/&amp;sa=D&amp;source=editors&amp;ust=1750774354950106&amp;usg=AOvVaw3vTV3AUd7kJwNUWveUuY3i">https://www.atlantic.net/gpu-server-hosting/the-best-gpus-for-scientific-research/</a></span></li><li class="c1 li-bullet-0"><span class="c11">ROCm-Aware Leader-based Designs for MPI Neighbourhood Collectives - Queen&#39;s University, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.queensu.ca/academia/afsahi/pprl/papers/ISC-2024.pdf&amp;sa=D&amp;source=editors&amp;ust=1750774354950845&amp;usg=AOvVaw3w8I1nZ61w0aQdaUFA2DRJ">https://www.queensu.ca/academia/afsahi/pprl/papers/ISC-2024.pdf</a></span></li><li class="c1 li-bullet-0"><span class="c11">Understanding Data Movement in AMD Multi-GPU Systems with Infinity Fabric | Request PDF - ResearchGate, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.researchgate.net/publication/387829898_Understanding_Data_Movement_in_AMD_Multi-GPU_Systems_with_Infinity_Fabric&amp;sa=D&amp;source=editors&amp;ust=1750774354951670&amp;usg=AOvVaw0poVBJch-zi5dBLfJOfMGV">https://www.researchgate.net/publication/387829898_Understanding_Data_Movement_in_AMD_Multi-GPU_Systems_with_Infinity_Fabric</a></span></li><li class="c1 li-bullet-0"><span class="c11">WHERE AMD INSTINCT&trade; GPUs WIN, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/content/dam/amd/en/documents/partner-hub/instinct/instinct-gpus-win-ebook.pdf&amp;sa=D&amp;source=editors&amp;ust=1750774354952344&amp;usg=AOvVaw24j0Yo9u_jXMqa_enE1G8v">https://www.amd.com/content/dam/amd/en/documents/partner-hub/instinct/instinct-gpus-win-ebook.pdf</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Instinct&trade; MI200 Series Accelerators, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/products/accelerators/instinct/mi200.html&amp;sa=D&amp;source=editors&amp;ust=1750774354952937&amp;usg=AOvVaw3sjnOzmmLO3_xfrKDfFEs3">https://www.amd.com/en/products/accelerators/instinct/mi200.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">NVIDIA 3080 GPU not working on ALIENWARE M17 R5 AMD - Please don&#39;t tell me it&#39;s dead, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/AlienwareTechsupport/comments/1kspg15/nvidia_3080_gpu_not_working_on_alienware_m17_r5/&amp;sa=D&amp;source=editors&amp;ust=1750774354953777&amp;usg=AOvVaw06EyYd5_jICfXx15iTo_ZB">https://www.reddit.com/r/AlienwareTechsupport/comments/1kspg15/nvidia_3080_gpu_not_working_on_alienware_m17_r5/</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD To Focus On Better ROCm Linux Experience In H2-2025, Day-One Client Support, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.phoronix.com/news/AMD-ROCm-H2-2025&amp;sa=D&amp;source=editors&amp;ust=1750774354954566&amp;usg=AOvVaw3NfHlhAq5x4zKOpaamitX3">https://www.phoronix.com/news/AMD-ROCm-H2-2025</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD Support | Drivers, Software &amp; Product Resources, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/support.html&amp;sa=D&amp;source=editors&amp;ust=1750774354955159&amp;usg=AOvVaw3Cq30t6QH0Og_tkVYixZ2U">https://www.amd.com/en/support.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">HPC Solutions - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.amd.com/en/solutions/high-performance-computing.html&amp;sa=D&amp;source=editors&amp;ust=1750774354955670&amp;usg=AOvVaw1P1-pMQa0ZRNtHa1j9KbXE">https://www.amd.com/en/solutions/high-performance-computing.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">All Posts - AMD ROCm&trade; Blogs, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.blogs.amd.com/blog.html&amp;sa=D&amp;source=editors&amp;ust=1750774354956099&amp;usg=AOvVaw1qGTN5Z5KZGPw9NXAOiimR">https://rocm.blogs.amd.com/blog.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Contributing to ROCm-DS, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/projects/rocm-ds/en/docs-25.05/contribute/contributing.html&amp;sa=D&amp;source=editors&amp;ust=1750774354956779&amp;usg=AOvVaw15v_Vfnx-YmFQFdOAydwAI">https://rocm.docs.amd.com/projects/rocm-ds/en/docs-25.05/contribute/contributing.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Providing feedback about the ROCm documentation - AMD, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/en/develop/contribute/feedback.html&amp;sa=D&amp;source=editors&amp;ust=1750774354957247&amp;usg=AOvVaw11oKsRKTvGMIgLAEljRHyq">https://rocm.docs.amd.com/en/develop/contribute/feedback.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AMD 2.0 &ndash; New Sense of Urgency | MI450X Chance to Beat Nvidia - SemiAnalysis, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/&amp;sa=D&amp;source=editors&amp;ust=1750774354957948&amp;usg=AOvVaw3O4d4tjUx-LlQwDkd2eI4v">https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/</a></span></li><li class="c1 li-bullet-0"><span class="c11">Data Center GPUs Market Research Report 2024-2025 &amp; 2034 - - GlobeNewswire, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.globenewswire.com/news-release/2025/05/20/3084707/28124/en/Data-Center-GPUs-Market-Research-Report-2024-2025-2034-HPC-Market-Expansion-Creates-Major-Opportunities-for-GPU-Server-Leaders-Like-Dell-and-Google.html&amp;sa=D&amp;source=editors&amp;ust=1750774354959020&amp;usg=AOvVaw1vatkLXDc6Xi-5fmqfpfMR">https://www.globenewswire.com/news-release/2025/05/20/3084707/28124/en/Data-Center-GPUs-Market-Research-Report-2024-2025-2034-HPC-Market-Expansion-Creates-Major-Opportunities-for-GPU-Server-Leaders-Like-Dell-and-Google.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">AI chip shortages deepen amid tariff risks - Sourceability, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://sourceability.com/post/ai-chip-shortages-deepen-amid-tariff-risks&amp;sa=D&amp;source=editors&amp;ust=1750774354959675&amp;usg=AOvVaw2HjepcZQ-amhDLhx9A2TaR">https://sourceability.com/post/ai-chip-shortages-deepen-amid-tariff-risks</a></span></li><li class="c1 li-bullet-0"><span class="c11">Experience Next-Gen HPC Innovation: AMD Lab Empowers &#39;Try Before You Buy&#39; on Azure, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hpcwire.com/solution_content/microsoft-amd/experience-next-gen-hpc-innovation-amd-lab-empowers-try-before-you-buy-on-azure/&amp;sa=D&amp;source=editors&amp;ust=1750774354960423&amp;usg=AOvVaw3N1B5yAAybGfIs3En8mKa_">https://www.hpcwire.com/solution_content/microsoft-amd/experience-next-gen-hpc-innovation-amd-lab-empowers-try-before-you-buy-on-azure/</a></span></li><li class="c1 li-bullet-0"><span class="c11">leveraging hip on intel pvc | ixpug, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.ixpug.org/resources/download/leveraging-hip-on-intel-pvc&amp;sa=D&amp;source=editors&amp;ust=1750774354961032&amp;usg=AOvVaw1pPz_g7bH8T-BBX9eaoe76">https://www.ixpug.org/resources/download/leveraging-hip-on-intel-pvc</a></span></li><li class="c1 li-bullet-0"><span class="c11">Porting CUDA Applications to Run on AMD GPUs - HPCwire, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.hpcwire.com/2022/11/28/porting-cuda-applications-to-run-on-amd-gpus/&amp;sa=D&amp;source=editors&amp;ust=1750774354961600&amp;usg=AOvVaw1RbQjqqI9oUJfq6H5OrF-X">https://www.hpcwire.com/2022/11/28/porting-cuda-applications-to-run-on-amd-gpus/</a></span></li><li class="c1 li-bullet-0"><span class="c11">HIP error codes &mdash; HIP 6.4.43483 Documentation - ROCm Documentation, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://rocm.docs.amd.com/projects/HIP/en/latest/reference/error_codes.html&amp;sa=D&amp;source=editors&amp;ust=1750774354962197&amp;usg=AOvVaw2O8uFM_eT_m0VUWuXQqA8Z">https://rocm.docs.amd.com/projects/HIP/en/latest/reference/error_codes.html</a></span></li><li class="c1 li-bullet-0"><span class="c11">Minimal requirements for ROCm support of AMD GPUs - Reddit, accessed June 24, 2025, </span><span class="c2"><a class="c4" href="https://www.google.com/url?q=https://www.reddit.com/r/ROCm/comments/1ahcf38/minimal_requirements_for_rocm_support_of_amd_gpus/&amp;sa=D&amp;source=editors&amp;ust=1750774354962873&amp;usg=AOvVaw3tRuL4Dqw_9V079X-hf--X">https://www.reddit.com/r/ROCm/comments/1ahcf38/minimal_requirements_for_rocm_support_of_amd_gpus/</a></span></li></ol></body></html>